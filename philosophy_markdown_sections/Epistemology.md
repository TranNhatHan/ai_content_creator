# Epistemology

## Dialectical method

BEFORE

c.600–450 BCE Pre-Socratic philosophers in Ionia and Italy attempt to explain the nature of the cosmos.

Early 5th century BCE Parmenides states that we can only understand the universe through reasoning.

c.450 BCE Protagoras and the Sophists apply rhetoric to philosophical questions.

AFTER

c.399–355 BCE Plato portrays the character of Socrates in the Apology and numerous other dialogues.

4th century BCE Aristotle acknowledges his debt to Socrates’ method.

Socrates is often referred to as one of the founders of Western philosophy, and yet he wrote nothing, established no school, and held no particular theories of his own. What he did do, however, was persistently ask the questions that interested him, and in doing so evolved a new way of thinking, or a new way of examining what we think. This has been called the Socratic, or dialectical, method (“dialectical” because it proceeds as a dialogue between opposing views), and it earned him many enemies in Athens, where he lived. He was vilified as a Sophist (someone who argues for the sake of deception), and was sentenced to death on charges of corrupting the young with ideas that undermined tradition. But he also had many followers, and among them was Plato, who recorded Socrates’ ideas in a series of written works, called dialogues, in which Socrates sets about examining various ideas. It is largely thanks to these dialogues—which include the Apology, Phaedo, and the Symposium—that Socrates’ thought survived at all, and that it went on to guide the course of Western philosophy.

The purpose of life

Socrates lived in Athens in the second half of the 5th century BCE. As a young man he is believed to have studied natural philosophy, looking at the various explanations of the nature of the universe, but then became involved in the politics of the city-state and concerned with more down-to-earth ethical issues, such as the nature of justice. However, he was not interested in winning arguments, or arguing for the sake of making money—a charge that was leveled at many of his contemporaries. Nor was he seeking answers or explanations—he was simply examining the basis of the concepts we apply to ourselves (such as “good”, “bad”, and “just”), for he believed that understanding what we are is the first task of philosophy.

Socrates’ central concern, then, was the examination of life, and it was his ruthless questioning of people’s most cherished beliefs (largely about themselves) that earned him his enemies—but he remained committed to his task until the very end. According to the account of his defence at his trial, recorded by Plato, Socrates chose death rather than face a life of ignorance: “The life which is unexamined is not worth living.”

But what exactly is involved in this examination of life? For Socrates it was a process of questioning the meaning of essential concepts that we use every day but have never really thought about, thereby revealing their real meaning and our own knowledge or ignorance. Socrates was one of the first philosophers to consider what it was that constituted a “good” life; for him it meant achieving peace of mind as a result of doing the right thing, rather than living according to the moral codes of society. And the “right thing” can only be determined through rigorous examination.

Socrates rejected the notion that concepts such as virtue were relative, insisting instead that they were absolutes, applicable not just to citizens of Athens, or Greece, but to all people in the world. He believed that virtue (areté in Greek, which at the time implied excellence and fulfilment) was “the most valuable of possessions”, and that no-one actually desires to do evil. Anyone performing evil actions would be acting against their conscience and would therefore feel uncomfortable; and as we all strive for peace of mind it is not something we would do willingly. Evil, he thought, was done because of lack of wisdom and knowledge. From this he concluded that “there is only one good: knowledge; and one evil: ignorance.” Knowledge is inextricably bound to morality—it is the “only one good”—and for this reason we must continually “examine” our lives.

"I am a citizen of the world."

Socrates

Socrates’ dialectical method was a simple method of questioning that brought to light the often false assumptions on which particular claims to knowledge are based.

Care of the soul

For Socrates, knowledge may also play a part in life after death. In the Apology, Plato’s Socrates prefaces his famous quote about the unexamined life by saying: “I tell you that to let no day pass without discussing goodness and all the other subjects about which you hear me talking, and that examining both myself and others is really the very best thing a man can do.” This gaining of knowledge, rather than wealth or high status, is the ultimate goal of life. It is not a matter of entertainment or curiosity—it is the reason why we exist. Moreover, all knowledge is ultimately self-knowledge, for it creates the person you are within this world, and fosters the care of the immortal soul.

In Phaedo, Socrates says that an unexamined life leads the soul to be “confused and dizzy, as if it were drunk”, while the wise soul achieves stability, its straying finally brought to an end.

Dialectical method

Socrates quickly became a well-known figure in Athens, with a reputation for an enquiring mind. A friend of his, so the story goes, asked the priestess of Apollo at Delphi who the wisest man in the world was: the oracular reply was that there was no-one wiser than Socrates. When Socrates heard about this, he was astounded, and went to the most knowledgeable people he could find to try to disprove it. What he discovered was that these people only thought they knew a great deal; under examination, their knowledge was proved to be either limited or false.

What was more important, however, was the method he used to question their knowledge. He took the standpoint of someone who knew nothing, and merely asked questions, exposing contradictions in arguments and gaps in knowledge to gradually elicit insights. He likened the process to his mother’s profession of midwife, assisting in the birth of ideas.

Through these discussions, Socrates came to realize that the Delphic oracle had been right—he was the wisest man in Athens, not because of his knowledge but because he professed to know nothing. He also saw that the inscription on the entrance to the temple at Delphi, gnothi seauton (“know thyself”), was just as significant. To gain knowledge of the world and oneself it was necessary to realize the limits of one’s own ignorance and to remove all preconceptions. Only then could one hope to determine the truth.

Socrates set about engaging the people of Athens in discussion on topics such as the nature of love, justice, and loyalty. His mission, misunderstood at the time as a dangerous form of Sophistry—or cleverness for the sake of it—was not to instruct the people, nor even simply to learn what they knew, but to explore the ideas that they had. It was the conversation itself, with Socrates guiding it, that provided him with insights. Through a series of questions, he revealed the ideas and assumptions his opponent held, then exposed the contradictions within them and brought them to agree to a new set of conclusions.

This method of examining an argument by rational discussion from a position of ignorance marked a complete change in philosophical thinking. It was the first known use of inductive argument, in which a set of premises based on experience is first established to be true, and then shown to lead to a universal truth in conclusion. This powerful form of argument was developed by Aristotle, and later by Francis Bacon, who used it as the starting point of his scientific method. It became, therefore, the foundation not only of Western philosophy, but of all the empirical sciences.

"I know nothing except the fact of my ignorance."

Socrates

Socrates was put to death in 399 BCE, ultimately for questioning the basis of Athenian morality. Here he accepts the bowl of hemlock that will kill him, and gestures defiantly at the heavens.

SOCRATES

Born in Athens in 469 BCE, Socrates was the son of a stonemason and a midwife. It is likely that he pursued his father’s profession, and had the opportunity to study philosophy, before he was called up for military service. After distinguishing himself during the Peloponnesian War, he returned to Athens, and for a while involved himself in politics. However, when his father died he inherited enough money to live with his wife Xanthippe without having to work.

From then on, Socrates became a familiar sight around Athens, involving himself in philosophical discussions with fellow citizens and gaining a following of young students. He was eventually accused of corrupting the minds of young Athenians, and was sentenced to death. Although he was offered the choice of exile, he accepted the guilty verdict and was given a fatal dose of hemlock in 399 BCE.

Key works

4th–3rd century BCE Plato’s record of Socrates’ life and philosophy in the Apology and numerous dialogues.

## Epistemology

BEFORE

6th century BCE The Milesian philosophers propose theories to explain the nature and substance of the cosmos.

c.500 BCE Heraclitus argues that everything is constantly in a state of flux or change.

c.450 BCE Protagoras says that truth is relative.

AFTER

c.335 BCE Aristotle teaches that we can find truth by observing the world around us.

c.250 CE Plotinus founds the Neo-Platonist school, a religious take on Plato’s ideas.

386 St. Augustine of Hippo integrates Plato’s theories into Christian doctrine.

In 399 BCE, Plato’s mentor Socrates was condemned to death. Socrates had left no writings, and Plato took it upon himself to preserve what he had learnt from his master for posterity—first in the Apology, his retelling of Socrates’ defense at his trial, and later by using Socrates as a character in a series of dialogues. In these dialogues, it is sometimes difficult to untangle which are Socrates’ thoughts and which are the original thoughts of Plato, but a picture emerges of Plato using the methods of his master to explore and explain his own ideas.

Initially Plato’s concerns were very much those of his mentor: to search for definitions of abstract moral values such as “justice” and “virtue”, and to refute Protagoras’s notion that right and wrong are relative terms. In the Republic, Plato set out his vision of the ideal city-state and explored aspects of virtue. But in the process, he also tackled subjects outside moral philosophy. Like earlier Greek thinkers, he questioned the nature and substance of the cosmos, and explored how the immutable and eternal could exist in a seemingly changing world. However, unlike his predecessors, Plato concluded that the “unchanging” in nature is the same as the “unchanging” in morals and society.

Seeking the Ideal

In the Republic, Plato describes Socrates posing questions about the virtues, or moral concepts, in order to establish clear and precise definitions of them. Socrates had famously said that “virtue is knowledge”, and that to act justly, for example, you must first ask what justice is. Plato decides that before referring to any moral concept in our thinking or reasoning, we must first explore both what we mean by that concept and what makes it precisely the kind of thing that it is. He raises the question of how we would recognize the correct, or perfect, form of anything—a form that is true for all societies and for all time. By doing so, Plato is implying that he thinks some kind of ideal form of things in the world we inhabit—whether those things are moral concepts or physical objects—must actually exist, of which we are in some way aware.

Plato talks about objects in the world around us, such as beds. When we see a bed, he states, we know that it is a bed and we can recognize all beds, even though they may differ in numerous ways. Dogs in their many species are even more varied, yet all dogs share the characteristic of “dogginess”, which is something we can recognize, and that allows us to say we know what a dog is. Plato argues that it is not just that a shared “dogginess” or “bedness” exists, but that we all have in our minds an idea of an ideal bed or dog, which we use to recognize any particular instance.

Taking a mathematical example to further his argument, Plato shows that true knowledge is reached by reasoning, rather than through our senses. He states that we can work out in logical steps that the square of the hypotenuse of a right-angled triangle is equal to the sum of the squares of the other two sides, or that the sum of the three interior angles of any triangle is always 180 degrees. We know the truth of these statements, even though the perfect triangle does not exist anywhere in the natural world. Yet we are able to perceive the perfect triangle—or the perfect straight line or circle—in our minds, using our reason. Plato, therefore, asks whether such perfect forms can exist anywhere.

World of Ideas

Reasoning brings Plato to only one conclusion—that there must be a world of Ideas, or Forms, which is totally separate from the material world. It is there that the Idea of the perfect “triangle”, along with the Idea of the perfect “bed” and “dog” exists. He concludes that human senses cannot perceive this place directly—it is only perceptible to us through reason. Plato even goes on to state that this realm of Ideas is “reality”, and that the world around us is merely modelled upon it.

To illustrate his theory, Plato presents what has become known as the “Allegory of the Cave.” He asks us to imagine a cave in which people have been imprisoned since birth, tied up facing the back wall in the darkness. They can only face straight ahead. Behind the prisoners is a bright fire, which casts shadows onto the wall they are facing. There is also a rampart between the fire and the prisoners along which people walk and hold up various objects from time to time, so that the shadows of these objects are cast on the wall. These shadows are all the prisoners know of the world; they have no concept of the actual objects themselves. If one of the prisoners manages to untie himself and turn around, he will see the objects themselves. But after a lifetime of entrapment, he is likely to be confused, as well as dazzled by the fire, and will most likely turn back toward the wall and the only reality he knows.

Plato believes that everything that our senses perceive in the material world is like the images on the cave wall, merely shadows of reality. This belief is the basis of his theory of Forms, which is that for every earthly thing that we have the power to perceive with our senses, there is a corresponding “Form” (or “Idea”)—an eternal and perfect reality of that thing—in the world of Ideas. Because what we perceive via our senses is based on an experience of imperfect or incomplete “shadows” of reality, we can have no real knowledge of those things. At best, we may have opinions, but genuine knowledge can only come from study of the Ideas, and that can only ever be achieved through reason, rather than through our deceptive senses.

This separation of two distinct worlds, one of appearance, the other of what Plato considers to be reality, also solves the problem of finding constants in an apparently changing world. The material world may be subject to change, but Plato’s world of Ideas is eternal and immutable. Plato applies his theory not just to concrete things, such as beds and dogs, but also to abstract concepts. In Plato’s world of Ideas, there is an Idea of justice, which is true justice, and all the instances of justice in the material world around us are models, or lesser variants, of it. The same is true of the concept of goodness, which Plato considers to be the ultimate Idea—and the goal of all philosophical enquiry.

"If particulars are to have meaning, there must be universals."

Plato

The Allegory of the Cave, in which knowledge of the world is limited to mere shadows of reality and truth, is used by Plato to explain his idea of a world of perfect Forms, or Ideas.

According to Plato’s theory of Forms, every horse that we encounter in the world around us is a lesser version of an “ideal”, or perfect, horse that exists in a world of Forms or Ideas—a realm that humans can only access through their ability to reason.

Innate knowledge

The problem remains of how we can come to know these Ideas, so that we have the ability to recognize the imperfect instances of them in the world we inhabit. Plato argues that our conception of Ideal Forms must be innate, even if we are not aware of this. He believes that human beings are divided into two parts: the body and the soul. Our bodies possess the senses, through which we are able to perceive the material world, while the soul possesses the reason with which we can perceive the realm of Ideas. Plato concludes that our soul, which is immortal and eternal, must have inhabited the world of Ideas before our birth, and still yearns to return to that realm after our death. So when we see variations of the Ideas in the world with our senses, we recognize them as a sort of recollection. Recalling the innate memories of these Ideas requires reason—an attribute of the soul.

For Plato, the philosopher’s job is to use reason to discover the Ideal Forms or Ideas. In the Republic, he also argues that it is philosophers, or rather those who are true to the philosopher’s calling, who should be the ruling class. This is because only the true philosopher can understand the exact nature of the world and the truth of moral values. However, just like a prisoner in the “Allegory of the Cave” who sees the real objects rather than their shadows, many will just turn back to the only world they feel comfortable with. Plato often found it difficult to convince his fellow philosophers of the true nature of their calling.

"The soul of man is immortal and imperishable."

Plato

Marcus Aurelius, Roman Emperor from 161 to 180 CE, was not just a powerful ruler, he was a noted scholar and thinker—a realization of Plato’s idea that philosophers should lead society.

Unsurpassed legacy

Plato himself was the embodiment of his ideal, or true, philosopher. He argued on questions of ethics that had been raised previously by the followers of Protagoras and Socrates, but in the process, he explored for the first time the path to knowledge itself. He was a profound influence on his pupil Aristotle—even if they fundamentally disagreed about the theory of Forms. Plato’s ideas later found their way into the philosophy of medieval Islamic and Christian thinkers, including St. Augustine of Hippo, who combined Plato’s ideas with those of the Church.

By proposing that the use of reason, rather than observation, is the only way to acquire knowledge, Plato also laid the foundations of 17th-century rationalism. Plato’s influence can still be felt today—the broad range of subjects he wrote about led the 20th-century British logician Alfred North Whitehead to say that subsequent Western philosophy “consists of a set of footnotes to Plato.”

"What we call learning is only a process of recollection."

Plato

PLATO

Despite the large proportion of writings attributed to Plato that have survived, little is known about his life. He was born into a noble family in Athens in around 427 BCE and named Aristocles, but acquired the nickname “Plato” (meaning “broad”). Although probably destined for a life in politics, he became a pupil of Socrates. When Socrates was condemned to death, Plato is said to have become disillusioned with Athens, and left the city. He travelled widely, spending some time in southern Italy and Sicily, before returning to Athens around 385 BCE. Here he founded a school known as the Academy (from which the word “academic” comes), remaining its head until his death in 347 BCE.

Key works

c.399–387 BCE Apology, Crito, Giorgias, Hippias Major, Meno, Protagoras (early dialogues)

c.380–360 BCE Phaedo, Phaedrus, Republic, Symposium (middle dialogues)

c.360–355 BCE Parmenides, Sophist, Theaetetus (late dialogues)

## Epistemology

BEFORE

399 BCE Socrates argues that virtue is wisdom.

c.380 BCE Plato presents his theory of Forms in his Socratic dialogue, The Republic.

AFTER

9th century CE Aristotle’s writings are translated into Arabic.

13th century Translations of Aristotle’s works appear in Latin.

1690 John Locke establishes a school of British empiricism.

1735 Zoologist Carl Linnaeus lays the foundations of modern taxonomy in Systema Naturae, based on Aristotle’s system of biological classification.

Aristotle was 17 years old when he arrived in Athens to study at the Academy under the great philosopher Plato. Plato himself was 60 at the time, and had already devised his theory of Forms. According to this theory, all earthly phenomena, such as justice and the color green, are shadows of ideal counterparts, called Forms, which give their earthly models their particular identities.

Aristotle was a studious type, and no doubt learnt a great deal from his master, but he was also of a very different temperament. Where Plato was brilliant and intuitive, Aristotle was scholarly and methodical. Nevertheless, there was an obvious mutual respect, and Aristotle stayed at the Academy, both as a student and a teacher, until Plato died 20 years later. Surprisingly, he was not chosen as Plato’s successor, and so he left Athens and took what would prove to be a fruitful trip to Ionia.

Plato’s theory questioned

The break from teaching gave Aristotle the opportunity to indulge his passion for studying wildlife, which intensified his feeling that Plato’s theory of Forms was wrong. It is tempting to imagine that Aristotle’s arguments had already had some influence on Plato, who in his later dialogues admitted some flaws in his earlier theories, but it is impossible to know for certain. We do know, though, that Plato was aware of the Third Man argument, which Aristotle used to refute his theory of Forms. This argument runs as follows: if there exists in a realm of Forms a perfect Form of Man on which earthly men are modelled, this Form, to have any conceivable content, would have to be based on a Form of the Form of Man—and this too would have to be based on a higher Form on which the Forms of the Forms are based, and so on ad infinitum.

Aristotle’s later argument against the theory of Forms was more straightforward, and more directly related to his studies of the natural world. He realized that it was simply unnecessary to assume that there is a hypothetical realm of Forms, when the reality of things can already be seen here on Earth, inherent in everyday things.

Perhaps because his father had been a physician, Aristotle’s scientific interests lay in what we now call the biological sciences, whereas Plato’s background had been firmly based in mathematics. This difference in background helps to explain the difference in approach between the two men. Mathematics, especially geometry, deals with abstract concepts that are far removed from the everyday world, whereas biology is very much about the world around us, and is based almost solely on observation. Plato sought confirmation of a realm of Forms from notions such as the perfect circle (which cannot exist in nature), but Aristotle found that certain constants can be discovered by examining the natural world.

Plato and Aristotle differed in their opinion of the nature of universal qualities. For Plato, they reside in the higher realm of the Forms, but for Aristotle they reside here on Earth.

Trusting the senses

What Aristotle proposed turned Plato’s theory on its head. Far from mistrusting our senses, Aristotle relied on them for the evidence to back up his theories. What he learnt from studying the natural world was that by observing the characteristics of every example of a particular plant or animal that he came across, he could build up a complete picture of what it was that distinguished it from other plants or animals, and deduce what makes it what it is. His own studies confirmed what he already believed—that we are not born with an innate ability to recognize Forms, as Plato maintained.

Each time a child comes across a dog, for example, it notes what it is about that animal that it has in common with other dogs, so that it can eventually recognize the things that make something a dog. The child now has an idea of “dogginess”, or the “form”, as Aristotle puts it, of a dog. In this way, we learn from our experience of the world what the shared characteristics are that make things what they are—and the only way of experiencing the world is through our senses.

"Everything that depends on the action of nature is by nature as good as it can be."

Aristotle

The essential form of things

Like Plato, then, Aristotle is concerned with finding some kind of immutable and eternal bedrock in a world characterized by change, but he concludes that there is no need to look for this anchor in a world of Forms that are only perceptible to the soul. The evidence is here in the world around us, perceptible through the senses. Aristotle believes that things in the material world are not imperfect copies of some ideal Form of themselves, but that the essential form of a thing is actually inherent in each instance of that thing. For example, “dogginess” is not just a shared characteristic of dogs—it is something that is inherent in each and every dog.

By studying particular things, therefore, we can gain insight into their universal, immutable nature.

What is true of examples in the natural world, Aristotle reasons, is also true of concepts relating to human beings. Notions such as “virtue”, “justice”, “beauty”, and “good” can be examined in exactly the same way. As he sees it, when we are born our minds are like “unscribed tablets”, and any ideas that we gain can only be received through our senses. At birth, we have no innate ideas, so we can have no idea of right or wrong. As we encounter instances of justice throughout our lives, however, we learn to recognize the qualities that these instances have in common, and slowly build and refine our understanding of what justice is. In other words, the only way we can come to know the eternal, immutable idea of justice, is by observing how it is manifested in the world around us.

Aristotle departs from Plato, then, not by denying that universal qualities exist, but by questioning both their nature and the means by which we come to know them (the latter being the fundamental quesion of “epistemology”, or the theory of knowledge). And it was this difference of opinion on how we arrive at universal truths that later divided philosophers into two separate camps: the rationalists (including René Descartes, Immanuel Kant, and Gottfried Leibniz), who believe in a priori, or innate, knowledge; and the empiricists (including John Locke, George Berkeley, and David Hume), who claim that all knowledge comes from experience.

Aristotle classified many of the different strands of knowledge and learning that we have today, such as physics, logic, metaphysics, poetics, ethics, politics, and biology.

"All men by nature desire to know."

Aristotle

Biological classification

The manner in which Plato and Aristotle arrive at their theories tells us much about their temperaments. Plato’s theory of Forms is grand and otherworldly, which is reflected in the way he argues his case, using highly imaginative fictionalized dialogues between Socrates and his contemporaries. By contrast, Aristotle’s theory is much more down to earth, and is presented in more prosaic, academic language. Indeed, so convinced was Aristotle that the truth of the world is to be found here on Earth, and not in some higher dimension, that he set about collecting specimens of flora and fauna, and classified them according to their characteristics.

For this biological classification, Aristotle devised a hierarchical system—the first of its kind, and so beautifully constructed that it forms the basis of the taxonomy still in use today. First, he divides the natural world into living and non-living things, then he turns his attention to classifying the living world. His next division is between plants and animals, which involves the same kind of thinking that underpins his theory of universal qualities: we may be able to distinguish between a plant and an animal almost without thinking, but how do we know how to make that distinction? The answer, for Aristotle, is in the shared features of either category. All plants share the form “plant”, and all animals share the form “animal.” And once we understand the nature of those forms, we can then recognize them in each and every instance.

This fact becomes more apparent the more Aristotle subdivides the natural world. In order to classify a specimen as a fish, for example, we have to recognize what it is that makes a fish a fish—which, again, can be known through experience and requires no innate knowledge at all. As Aristotle builds up a complete classification of all living things, from the simplest organisms to human beings, this fact is confirmed again and again.

Teleological explanation

Another fact that became obvious to Aristotle as he classified the natural world is that the “form” of a creature is not just a matter of its physical characteristics, such as its skin, fur, feather, or scales, but also a matter of what it does, and how it behaves—which, for Aristotle, has ethical implications.

To understand the link with ethics, we need first to appreciate that for Aristotle everything in the world is fully explained by four causes that fully account for a thing’s existence. These four causes are: the material cause, or what a thing is made of; the formal cause, or the arrangement or shape of a thing; the efficient cause, or how a thing is brought into being; and the final cause, or the function or purpose of a thing. And it is this last type of cause, the “final cause”, that relates to ethics—a subject which, for Aristotle, is not separate from science, but rather a logical extension of biology.

An example that Aristotle gives is that of an eye: the final cause of an eye—its function—is to see. This function is the purpose, or telos, of the eye—telos is a Greek word that gives us “teleology”, or the study of purpose in nature. A teleological explanation of a thing is therefore an account of a thing’s purpose, and to know the purpose of a thing is also to know what a “good” or a “bad” version of a thing is—a good eye for example, is one that sees well.

In the case of humans, a “good” life is therefore one in which we fulfill our purpose, or use all the characteristics that make us human to the full. A person can be considered “good” if he uses the characteristics he was born with, and can only be happy by using all his capabilities in the pursuit of virtue—the highest form of which, for Aristotle, is wisdom. Which brings us full circle back to the question of how we can recognize the thing that we call virtue—and for Aristotle, again, the answer is by observation. We understand the nature of the “good life” by seeing it in the people around us.

Aristotle’s classification of living things is the first detailed examination of the natural world. It proceeds from general observations about the characteristics shared by all animals, and then subdivides into ever more precise categories.

The syllogism

In the process of classification, Aristotle formulates a systematic form of logic which he applies to each specimen to determine whether it belongs to a certain category. For example, one of the characteristics common to all reptiles is that they are cold-blooded; so, if this particular specimen is warm-blooded, then it cannot be a reptile. Likewise, a characteristic common to all mammals is that they suckle their young; so, if this specimen is a mammal, it will suckle its young. Aristotle sees a pattern in this way of thinking—that of three propositions consisting of two premises and a conclusion, for example in the form: if As are Xs, and B is an A, then B is an X. The “syllogism”, as this form of reasoning is known, is the first formal system of logic ever devised, and it remained the basic model for logic up until the 19th century.

But the syllogism was more than simply a by-product of Aristotle’s systematic classification of the natural world. By using analytical reasoning in the form of logic, Aristotle realized that the power of reason was something that did not rely on the senses, and that it must therefore be an innate characteristic—part of what it is to be human. Although we have no innate ideas, we do possess this innate faculty, which is necessary for us to learn from experience. And as he applied this fact to his hierarchical system, he saw that the innate power of reason is what distinguishes us from all other living creatures, and placed us at the top of the hierarchy.

"Linnaeus and Cuvier have been my two gods, though in very different ways, but they were mere schoolboys to old Aristotle."

Charles Darwin

“Socrates is mortal” is the undeniable conclusion to the most famous syllogism in history. Aristotle’s syllogism—a simple deduction from two premises to a conclusion—was the first formal system of logic.

Decline of Classical Greece

The sheer scope of Aristotle’s ideas, and the revolutionary way in which he overturns Plato’s theory of Forms, should have ensured that his philosophy had a far greater impact than it did during his lifetime. That is not to say that his work was without fault—his geography and astronomy were flawed; his ethics supported the use of slaves and considered women to be inferior human beings; and his logic was incomplete by modern standards. However, what he got right amounted to a revolution both in philosophy and in science.

But Aristotle lived at the end of an era. Alexander the Great, whom he taught, died shortly before him, and so began the Hellenistic period of Greek history which saw a decline in Athens’ influence. The Roman Empire was becoming the dominant power in the Mediterranean, and the philosophy it adopted from Greece was that of the Stoics. The rival schools of Plato and Aristotle—Plato’s Academy and the Lyceum Aristotle founded in Athens—continued to operate, but they had lost their former eminence.

As a result of this neglect, many of Aristotle’s writings were lost. It is believed that he wrote several hundred treatises and dialogues explaining his theories, but all that remain are fragments of his work, mainly in the form of lectures and teacher’s notes. Luckily for posterity, these were preserved by his followers, and there is enough contained in them to give a picture of the full range of his work.

"Every action must be due to one or other of seven causes: chance, nature, compulsion, habit, reasoning, anger, or appetite."

Aristotle

Aristotle’s legacy

With the emergence of Islam in the 7th century CE, Aristotle’s works were translated into Arabic and spread throughout the Islamic world, becoming essential reading for Middle Eastern scholars such as Avicenna and Averroes. In Western Europe, however, Boethius’s Latin translation of Aristotle’s treatise on logic (made in the 6th century CE) remained the only work of Aristotle’s available until the 9th century CE, when all of Aristotle’s works began to be translated from Arabic into Latin. It was also at this time that his ideas were collected into the the books we know today—such as Physics, The Nicomachean Ethics, and the Organon. In the 13th century, Thomas Aquinas braved a ban on Aristotle’s work and integrated it into Christian philosophy, in the same way that St. Augustine had adopted Plato, and Plato and Aristotle came to lock horns again.

Aristotle’s notes on logic (laid out in the Organon) remained the standard text on logic until the emergence of mathematical logic in the 19th century. Likewise, his classification of living things dominated Western thinking throughout the Middle Ages, becoming the Christian scala naturae (the “ladder of nature”), or the Great Chain of Being. This depicted the whole of creation dominated by man, who stood second only to God. And during the Renaissance, Aristotle’s empirical method of enquiry held sway.

In the 17th century, the debate between empiricists and rationalists reached its zenith after René Descartes published his Discourse on the Method. Descartes, and Leibniz and Kant after him, chose the rationalist route; in response, Locke, Berkeley, and Hume lined up as the empiricist opposition. Again, the differences between the philosophers were as much about temperament as they were about substance—the Continental versus the English, the poetic versus the academic, the Platonic versus the Aristotelian. Although the debate died down in the 19th century, there has been a revival of interest in Aristotle in recent times, and a reappraisal of his significance. His ethics in particular have been of great appeal to modern philosophers, who have seen in his functional definition of “good” a key to understanding the way we use ethical language.

"There is nothing in the mind except was first in the senses."

John Locke

The influence of Aristotle on the history of thought can be seen in the Great Chain of Being, a medieval Christian depiction of life as a hierarchy in which with God presides over all.

ARISTOTLE

Born in Stagira, Chalcidice, in the northeast region of modern Greece, Aristotle was the son of a physician to the royal family of Macedon, and was educated as a member of the aristocracy. He was sent to Plato’s Academy in Athens at the age of 17, and spent almost 20 years there both as a student and a teacher. When Plato died, Aristotle left Athens for Ionia, and spent several years studying the wildlife of the area. He was then appointed tutor at the Macedonian court, where he taught the young Alexander the Great and continued his studies.

In 335 BCE he returned to Athens, encouraged by Alexander, and set up the Lyceum, a school to rival Plato’s. It was here that he did most of his writing, and formalized his ideas. After Alexander died in 323 BCE, anti-Macedonian feeling flared up in Athens, and Aristotle fled to Chalcis, on the island of Euboea, where he died the following year.

Key works

Organon, Physics (as compiled in book form in the 9th century).

## Epistemology

BEFORE

Late 5th century BCE Socrates states that seeking knowledge and truth is the key to a worthwhile life.

c.400 BCE Democritus and Leucippus conclude that the cosmos consists solely of atoms, moving in empty space.

AFTER

c.50 BCE Roman philosopher Lucretius writes De rerum natura, a poem exploring Epicurus’s ideas.

1789 Jeremy Bentham advocates the utilitarian idea of “the greatest happiness for the greatest number.”

1861 John Stuart Mill argues that intellectual and spiritual pleasures have more value than physical pleasures.

Epicurus grew up in a time when the philosophy of ancient Greece had already reached a pinnacle in the ideas of Plato and Aristotle. The main focus of philosophical thinking was shifting from metaphysics toward ethics—and also from political to personal ethics. Epicurus, however, found the seeds of a new school of thought in the quests of earlier philosophers, such as Socrates’ examination of the truth of basic human concepts and values.

Central to the philosophy that Epicurus developed is the view that peace of mind, or tranquillity, is the goal of life. He argues that pleasure and pain are the roots of good and evil, and qualities such as virtue and justice derive from these roots, as “it is impossible to live a pleasant life without living wisely, honorably, and justly, and it is impossible to live wisely, honorably, and justly without living pleasantly.” Epicurianism is often mistakenly interpreted as simply being about the pursuit of sensual pleasures. For Epicurus, the greatest pleasure is only attainable through knowledge and friendship, and a temperate life, with freedom from fear and pain.

Fear of death

One of the obstacles to enjoying the peace of a tranquil mind, Epicurus reasons, is the fear of death, and this fear is increased by the religious belief that if you incur the wrath of the gods, you will be severely punished in the afterlife. But rather than countering this fear by proposing an alternative state of immortality, Epicurus tries to explain the nature of death itself. He starts by proposing that when we die, we are unaware of our death, since our consciousness (our soul) ceases to exist at the point of death. To explain this, Epicurus takes the view that the entire universe consists of either atoms or empty space, as argued by the atomist philosophers Democritus and Leucippus. Epicurus then reasons that the soul could not be empty space, because it operates dynamically with the body, so it must be made up of atoms. He describes these atoms of the soul as being distributed around the body, but as being so fragile that they dissolve when we die, and so we are no longer capable of sensing anything. If you are unable to feel anything, mentally or physically, when you die, it is foolish to let the fear of death cause you pain while you are still alive.

Epicurus attracted a small but devoted following in his lifetime, but he was perceived as being dismissive of religion, which made him unpopular. His thinking was largely ignored by mainstream philosophy for centuries, but it resurfaced in the 18th century, in the ideas of Jeremy Bentham and John Stuart Mill. In revolutionary politics, the tenets of Epicureanism are echoed in the words of the United States’ Declaration of Independence: “life, liberty, and the pursuit of happiness.”

Terrifying images of the merciless god of death Thanatos were used to depict the pain and torment ancient Greeks might incur for their sins, both when they died and in the afterlife.

EPICURUS

Born to Athenian parents on the Aegean island of Samos, Epicurus was first taught philosophy by a disciple of Plato. In 323 BCE, Alexander the Great died and, in the political conflicts that followed, Epicurus and his family were forced to move to Colophon (now in Turkey). There he continued his studies with Nausiphanes, a follower of Democritus.

Epicurus taught briefly in Mytilene on the island of Lesbos, and in Lampsacus on the Greek mainland, before moving to Athens in 306 BCE. He founded a school, known as the The Garden, consisting of a community of friends and followers. There he set down in great detail the philosophy that was to become known as Epicureanism.

Despite frequent ill health, and often being in great pain, Epicurus lived to the age of 72. True to his beliefs, he described the last day of his life as a truly happy day.

Key works

Early 3rd century BCE On Nature Prinicipal Doctrines, Vatican Sayings

See: Democritus and Leucippus • Socrates • Plato • Aristotle • Jeremy Bentham • John Stuart Mill



BEFORE

Late 5th century BCE Socrates teaches that the ideal life is one spent in search of truth.

Early 4th century BCE Socrates’ pupil Antisthenes advocates an ascetic life, lived in harmony with nature.

AFTER

c.301 BCE Influenced by Diogenes, Zeno of Citium founds a school of Stoics.

4th century CE St. Augustine of Hippo denounces the often shameless behavior of the Cynics, although they become the model for several ascetic Christian orders.

1882 Friedrich Nietzsche refers to Diogenes and his ideas in The Gay Science.

Plato once described Diogenes as “a Socrates gone mad.” Although this was meant as an insult, it is not far from the truth. Diogenes shares Socrates’ passion for virtue and rejection of material comfort, but takes these ideas to the extreme. He argues that in order to lead a good life, or one that is worth living, it is necessary to free oneself from the external restrictions imposed by society, and from the internal discontentment that is caused by desire, emotion, and fear. This can be achieved, he states, by being content to live a simple life, governed by reason and natural impulses, rejecting conventions without shame, and renouncing the desire for property and comfort.

Diogenes was the first of a group of thinkers who became known as the Cynics, a term taken from the Greek kunikos, meaning “dog-like.” It reflects the determination of the Cynics to spurn all forms of social custom and etiquette, and instead live in as natural a state as possible. They asserted that the more one can do this, as Diogenes himself did by living a life of poverty with only an abandoned tub for shelter, the nearer one will be to leading the ideal life.

The happiest person, who in Diogenes’ phrase, “has the most”, is therefore someone who lives in accordance with the rhythms of the natural world, free from the conventions and values of civilized society, and “content with the least.”

Rejecting worldly values, Diogenes chose to live on the streets. He flouted convention, by eating only discarded scraps and dressing—when he actually bothered to do so—in filthy rags.

## Epistemology

BEFORE

c.380 BCE Plato states his thoughts on ethics and the city-state in The Republic.

4th century BCE Diogenes of Sinope lives in extreme poverty to demonstrate his Cynic principles.

AFTER

c.40–45 CE Roman statesman and philosopher Seneca the Younger continues the Stoic tradition in his Dialogues.

c.150–180 Roman emperor Marcus Aurelius writes his 12-volume Meditations on Stoic philosophy.

1584 Flemish humanist Justus Lipsius writes De Constantia, combining Stoicism with Christianity to found a school of Neo-Stoicism.

Two main schools of philosophical thought emerged after Aristotle’s death. These were the hedonistic, godless ethic of Epicurus, which had limited appeal, and the more popular and longer-lasting Stoicism of Zeno of Citium.

Zeno studied with a disciple of Diogenes of Sinope, the Cynic, and shared his no-nonsense approach to life. He had little patience with metaphysical speculation and came to believe that the cosmos was governed by natural laws that were ordained by a supreme lawgiver. Man, he declares, is completely powerless to change this reality, and in addition to enjoying its many benefits, man also has to accept its cruelty and injustice.

Free will

However, Zeno also declares that man has been given a rational soul with which to exercise free will. No one is forced to pursue a “good” life. It is up to the individual to choose whether to put aside the things over which he has little or no control, and be indifferent to pain and pleasure, poverty and riches. But if a person does so, Zeno is convinced that he will achieve a life that is in harmony with nature in all its aspects, good or bad, and live in accordance with the rulings of the supreme lawgiver.

Stoicism was to find favor across much of Hellenistic Greece. But it drew in even more followers in the expanding Roman empire, where it flourished as a basis for ethics—both personal and political—until it was supplanted by Christianity in the 6th century.

"Happiness is a good flow of life."

Zeno of Citium

## Epistemology

BEFORE

c.400 BCE In Gorgias, Plato argues that evil is not a thing, but an absence of something.

3rd century CE Plotinus revives Plato’s view of good and evil.

AFTER

c.520 Boethius uses an Augustinian theory of evil in The Consolation of Philosophy.

c.1130 Pierre Abélard rejects the idea that there are not evil things.

1525 Martin Luther, the German priest who inspired the Protestant reformation, publishes On the Bondage of the Will, arguing that the human will is not free.

Augustine was especially interested in the problem of evil. If God is entirely good and all-powerful, why is there evil in the world? For Christians such as Augustine, as well as for adherents of Judaism and Islam, this was, and remains, a central question. This is because it makes an obvious fact about the world—that it contains evil—into an argument against the existence of God.

Augustine is able to answer one aspect of the problem quite easily. He believes that although God created everything that exists, he did not create evil, because evil is not a thing, but a lack or deficiency of something. For example, the evil suffered by a blind man is that he is without sight; the evil in a thief is that he lacks honesty. Augustine borrowed this way of thinking from Plato and his followers.

An essential freedom

But Augustine still needs to explain why God should have created the world in such a way as to allow there to be these natural and moral evils, or deficiencies. His answer revolves around the idea that humans are rational beings. He argues that in order for God to create rational creatures, such as human beings, he had to give them freedom of will. Having freedom of will means being able to choose, including choosing between good and evil. For this reason God had to leave open the possibility that the first man, Adam, would choose evil rather than good. According to the Bible this is exactly what happened, as Adam broke God’s command not to eat fruit from the Tree of Knowledge.

In fact, Augustine’s argument holds even without referring to the Bible. Rationality is the ability to evaluate choices through the process of reasoning. The process is only possible where there is freedom of choice, including the freedom to choose to do wrong.

Augustine also suggests a third solution to the problem, asking us to see the world as a thing of beauty. He says that although there is evil in the universe, it contributes to an overall good that is greater than it could be without evil—just as discords in music can make a harmony more lovely, or dark patches add to the beauty of a picture.

A world without evil, Augustine says, would be a world without us—rational beings able to choose their actions. Just as for Adam and Eve, our moral choices allow for the possibility of evil.

Explaining natural evils

Since Augustine’s time, most Christian philosophers have tackled the problem of evil using one of his approaches, while their opponents, such as David Hume, have pointed to their weaknesses as arguments against Christianity. Calling sickness, for instance, an absence of health seems to be just playing with words: illness may be due to a deficiency of something, but the suffering of the sick person is real enough. And how are natural evils, such as earthquakes and plagues, explained?

Someone without a prior belief in God might still argue that the presence of evil in the world proves that there is no all-powerful and benevolent God. But for those who do already believe in God, Augustine’s arguments might hold the answer.

"What made Adam capable of obeying God’s commands also made him able to sin."

St. Augustine of Hippo

ST. AUGUSTINE OF HIPPO

Aurelius Augustine was born in 354 CE in Thagaste, a small provincial town in North Africa, to a Christian mother and a pagan father. He was educated to be a rhetorician, and he went on to teach rhetoric in his home town, and at Carthage, Rome, and Milan, where he occupied a prestigious position.

For a while Augustine followed Manichaeism—a religion that sees good and evil as dual forces that rule the universe—but under the influence of Archbishop Ambrose of Milan, he became attracted to Christianity. In 386, he suffered a spiritual crisis and underwent a conversion. He abandoned his career and devoted himself to writing Christian works, many of a highly philosophical nature. In 395 he became Bishop of Hippo, in North Africa, and he held this post for the rest of his life. He died in Hippo, aged 75, when the town was beseiged and sacked by the Vandals.

Key works

c.388–95 On Free Will

c.397–401 Confessions

c.413–27 On the City of God

## Epistemology

BEFORE

c.350 BCE Aristotle outlines the problems of claiming as true any statement about the outcome of a future event.

c.300 BCE Syrian philosopher Iamblichus says that what can be known depends upon the knower’s capacity.

AFTER

c.1250–70 Thomas Aquinas agrees with Boethius that God exists outside of time, and so is transcendent and beyond human understanding.

c.1300 John Duns Scotus says that human freedom rests on God’s own freedom to act, and that God knows our future, free actions by knowing his own, unchanging—but free—will.

The Roman philosopher Boethius was trained in the Platonist tradition of philosophy, and was also a Christian. He is famous for his solution to a problem that predates Aristotle: if God already knows what we are going to do in the future, how can we be said to have free will?

The best way to understand the dilemma is to imagine a situation in everyday life. For instance, this afternoon I might go to the cinema, or I might spend time writing. As it turns out, I go to the cinema. That being the case, it is true now (before the event) that I will go the cinema this afternoon. But if it is true now, then it seems that I do not really have the choice of spending the afternoon writing. Aristotle was the first to define this problem, but his answer to it is not very clear; he seems to have thought that a sentence such as “I shall go to the cinema this afternoon” is neither true nor false, or at least not in the same way as “I went to the cinema yesterday.”

A God beyond time

Boethius faced a harder version of the same problem. He believed that God knows everything; not only the past and the present, but also the future. So if I am going to go to the cinema this afternoon, God knows it now. It seems, therefore, that I am not really free to choose to spend the afternoon writing, since that would conflict with what God already knows.

Boethius solves the problem by arguing that the same thing can be known in different ways, depending on the nature of the knower. My dog, for instance, knows the sun only as something with qualities he can sense—by sight and touch. A person, however, can also reason about the category of thing the sun is, and may know which elements it is made of, its distance from Earth, and so on.

Boethius considers time in a similar kind of way. As we live in the flow of time, we can only know events as past (if they have occurred), present (if they are happening now), or future (if they will come to pass). We cannot know the outcome of uncertain future events. God, by contrast, is not in the flow of time. He lives in an eternal present, and knows what to us are past, present, and future in the same way that we know the present. And just as my knowledge that you are sitting now does not interfere with your freedom to stop, so too God’s knowledge of our future actions, as if they were present, does not stop them from being free.

Some thinkers today argue that since I have not yet decided whether I shall go to the cinema this afternoon, there is simply nothing to be known about it, so even a God who is all-knowing does not, and cannot, know if I shall go or not.

"Everything is known, not according to itself, but according to the capacity of the knower."

Boethius

Lady Philosophy and Boethius discuss free will, determinism, and God’s vision of the eternal present in his influential book, The Consolation of Philosophy.

BOETHIUS

Anicius Boethius was a Christian Roman aristocrat, born at a time when the Roman Empire was disintegrating and the Ostrogoths ruled Italy. He became an orphan at the age of seven and was brought up by an aristocratic family in Rome. He was extremely well educated, speaking fluent Greek and having an extensive knowledge of Latin and Greek literature and philosophy. He devoted his life to translating and commenting on Greek texts, especially Aristotle’s works on logic, until he was made chief adviser to the Ostrogothic king Theoderic. Some five years later he became a victim of court intrigue, was wrongly accused of treason, and sentenced to death. He wrote his most famous work, The Consolation of Philosophy, while in prison awaiting execution.

Key works

c.510 Commentaries on Aristotle’s “Categories”

c.513–16 Commentaries on Aristotle’s “On Interpretation”

c.523–26 The Consolation of Philosophy

## Epistemology

BEFORE

c.400 CE St. Augustine of Hippo argues for God’s existence through our grasp of unchanging truths.

1075 In his Monologion, Anselm develops Augustine’s proof of God’s existence.

AFTER

1260s Thomas Aquinas rejects Anselm’s Ontological Argument.

1640 René Descartes uses a form of Anselm’s Ontological Argument in his Meditations.

1979 American philosopher Alvin Plantinga reformulates Anselm’s Ontological Argument using a form of modal logic to establish its truth.

Although Christian thinkers believe as a matter of faith that God exists, in the Middle Ages they were keen to show that God’s existence could also be proved by rational argument. The Ontological Argument invented by Anselm—an 11th-century Italian philosopher who worked on the basis of Aristotelian logic, Platonic thinking, and his own genius—is probably the most famous of all.

Anselm imagines himself arguing with a Fool, who denies that God exists. The argument rests on an acceptance of two things: first, that God is “that than which nothing greater can be thought”, and second, that existence is superior to non-existence. By the end of the argument the Fool is forced to either take up a self-contradictory position or admit that God exists.

The argument has been accepted by many great philosophers, such as René Descartes and Baruch Spinoza. But there have been many others who took up the Fool’s side. One contemporary of Anselm’s, Gaunilo of Marmoutiers, said that we could use the same argument to prove that there exists somewhere a marvellous island, greater than any island that can be thought. In the 18th century Immanuel Kant objected that the argument treats existence as if it were an attribute of things—as if I might describe my jacket like this: “it’s green, made of tweed, and it exists.” Existing is not like being green: if it did not exist, there would be no jacket to be green or tweed.

Kant holds that Anselm is also wrong to say that what exists in reality as well as in the mind is greater than what exists in the mind alone, but other philosophers disagree. Is there not a sense in which a real painting is greater than the mental concept the painter has before he starts work?

"We believe that You [God] are that than which nothing greater can be thought."

St. Anselm

Anselm’s Ontological Argument was written in 1077–78, but acquired its title from the German philosopher Kant in 1781.

ST ANSELM

St Anselm of Canterbury was born in Aosta in Italy in 1033. He left home in his twenties to study at the monastery of Bec, in France, under an eminent logician, grammarian, and Biblical commentator named Lanfranc. Anselm became a monk of Bec in 1060, then prior, and eventually abbot in 1078. He travelled to England, and in 1093 was made Archbishop of Canterbury, despite his protestations of ill-health and lack of political skills. This position put him in conflict with the Anglo-Norman kings William II and Henry I, as he tried to uphold the Church against royal power. These disputes led to two periods of exile from England for Anselm, during which he visited the pope to plead the case for the English Church and his own removal from office. Ultimately reconciled with King Henry I, Anselm died in Canterbury aged 76.

Key works

1075–76 Monologion

1077–78 Proslogion

1095–98 Why did God become Man?

1080–86 On the Fall of the Devil

## Epistemology

BEFORE

1090s Abû Hâmid al-Ghazâlî launches an attack on Islamic Aristotelian philosophers.

1120s Ibn Bâjja (Avempace) establishes Aristotelian philosophy in Islamic Spain.

AFTER

1270 Thomas Aquinas criticizes the Averroists for accepting conflicting truths from Christianity and Aristotelian philosophy.

1340s Moses of Narbonne publishes commentaries on Averroes’ work.

1852 French philosopher Ernest Renan publishes a study of Averroes, on the basis of which he becomes an important influence on modern Islamic political thought.

Averroes worked in the legal profession; he was a qâdî (an Islamic judge) who worked under the Almohads, one of the strictest Islamic regimes in the Middle Ages. Yet he spent his nights writing commentaries on the work of an ancient pagan philosopher, Aristotle—and one of Averroes’ avid readers was none other than the Almohad ruler, Abû Yacqûb Yûsuf.

Averroes reconciles religion and philosophy through a hierarchical theory of society. He thinks that only the educated elite are capable of thinking philosophically, and everyone else should be obliged to accept the teaching of the Qur’an literally. Averroes does not think that the Qur’an provides a completely accurate account of the universe if read in this literal way, but says that it is a poetic approximation of the truth, and this is the most that the uneducated can grasp.

However, Averroes believes that educated people have a religious obligation to use philosophical reasoning. Whenever reasoning shows the literal meaning of the Qur’an to be false, Averroes says that the text must be “interpreted”; that is to say the obvious meaning of the words should be disregarded and the scientific theory demonstrated by Aristotelian philosophy accepted in its place.

"Philosophers believe that religious laws are necessary political arts."

Averroes

The immortal intellect

Averroes is willing to sacrifice some widely-held Islamic doctrines in order to maintain the compatibility of philosophy and religion. For instance, almost all Muslims believe that the universe has a beginning, but Averroes agrees with Aristotle that it has always existed, and says that there is nothing in the Qur’an to contradict this view. However, the resurrection of the dead, a basic tenet of Islam, is harder to include within an Aristotelian universe. Averroes accepts that we must believe in personal immortality, and that anyone who denies this is a heretic who should be executed. But he takes a different position from his predecessors by saying that Aristotle’s treatise On the Soul does not state that individual humans have immortal souls. According to Averroes’ interpretation, Aristotle claims that humanity is immortal only through a shared intellect. Averroes seems to be saying that there are truths discoverable by humans that hold good for ever, but that you and I as individuals will perish when our bodies die.

Later Averroists

Averroes’ advocacy of Aristotelian philosophy (if only for the elite) was shunned by his fellow Muslims. But his works, translated into Hebrew and Latin, had enormous influence in the 13th and 14th centuries. Scholars who supported the opinions of Aristotle and Averroes became known as Averroists, and they included Jewish scholars such as Moses of Narbonne, and Latin scholars such as Boethius of Dacia and Siger of Brabant. The Latin Averroists acccepted Aristotle as interpreted by Averroes as the truth according to reason—despite also affirming an apparently conflicting set of Christian “truths.” They have been described as advocating a “double truth” theory, but their view is rather that truth is relative to the context of enquiry.

Some Muslims did not view philosophy as a legitimate subject for study in the 12th century, but Averroes argued that it was essential to engage with religion critically and philosophically.

AVERROES

Ibn Rushd, known in Europe as Averroes, was born in 1126 in Cordoba, then part of Islamic Spain. He belonged to a family of distinguished lawyers and trained in law, science, and philosophy. His friendship with another doctor and philosopher, Ibn Tufayl, led to an introduction to the Caliph Abû Yacqûb Yûsuf, who appointed Averroes chief judge and later court physician. Abû Yacqûb also shared Averroes’ interest in Aristotle, and commissioned him to write a series of paraphrases of all Aristotle’s works, designed for non-specialists such as himself. Despite the increasingly liberal views of the Almohads, the public disapproved of Averroes’ unorthodox philosophy, and public pressure led to a banning of his books and personal exile in 1195. Reprieved two years later, Averroes returned to Cordoba but died the following year.

Key works

1179–80 Decisive Treatise

1179–80 The Incoherence of the Incoherence

c.1186 Great Commentary on Aristotle’s ‘On the Soul’

## Epistemology

BEFORE

c.400 CE The philosopher Pseudo-Dionysius establishes the tradition of Christian negative theology, which states that God is not being, but more than being.

860s John Scotus Eriugena suggests that God creates the universe from the nothing which is himself.

AFTER

1260s Thomas Aquinas moderates Maimonides’ negative theology in his Summa Theologiae.

Early 1300s Meister Eckhart develops his negative theology.

1840–50s Søren Kierkegaard claims that it is impossible to provide any form of external description of God.

Maimonides wrote on both Jewish law (in Hebrew) and Aristotelian thought (in Arabic). In both areas, one of his central concerns was to guard against anthropomorphizing God, which is the tendency to think about God in the same way as a human being. For Maimonides, the worst mistake of all is to take the Torah (the first part of the Hebrew Bible) as literal truth, and to think that God is a bodily thing. Anyone who thinks this, he says, should be excluded from the Jewish community. But in the Guide of the Perplexed, Maimonides pushes this idea to its farthest extent, developing a strand of thought known as “negative theology.” This already existed in Christian theology, and it focuses on describing God only in terms of what God is not.

God, Maimonides says, has no attributes. We cannot rightly say that God is “good” or “powerful.” This is because an attribute is either accidental (capable of change) or essential. One of my accidental attributes, for example, is that I am sitting; others are that I have gray hair and a long nose. But I would still be what I essentially am even if I were standing, red-haired, and had a snub-nose. Being human—that is, being a rational, mortal animal—is my essential attribute: it defines me. God, it is generally agreed, has no accidental attributes, because God is unchanging. In addition, says Maimonides, God cannot have any essential attributes either, because they would be defining, and God cannot be defined. So God has no attributes at all.

Speaking about God

Maimondes claims that we can say things about God, but they must be understood as telling us about God’s actions, rather than God’s being. Most discussions in the Torah should be understood in this way. So when we are told that “God is a creator”, we must understand this as stating what God does, rather than the sort of thing God is. If we were to consider the sentence “John is a writer”, we might normally take it to mean that being a writer is John’s profession. But Maimonides asks us to consider only what has been done: in this instance John has written words. The writing has been brought about by John but it does not tell us anything about him.

Maimonides also accepts that statements which seem to attribute qualities to God can be understood if they are taken as double negatives. “God is powerful”: should be taken to mean that God is not powerless. Imagine a game in which I think of a thing and tell you what it is not (it is not large, it is not red…) until you guess what it is. The difference in the case of God is that we have only the negations to guide us: we cannot say what God is.

The Mishneh Torah was a complete restatement of Jewish Oral Law, which Maimonides wrote in plain Hebrew so that “young and old” could know and understand all the Jewish observances.

"When the intellects contemplate God’s essence, their apprehension turns into incapacity."

Maimonides

MOSES MAIMONIDES

Moses Maimonides (also known as Rambam) was born in 1135 in Cordoba, Spain, into a Jewish family. His childhood was rich in cross-cultural influences: he was educated in both Hebrew and Arabic, and his father, a rabbinic judge, taught him Jewish law within the context of Islamic Spain. His family fled Spain when the Berber Almohad dynasty came to power in 1148, and lived nomadically for 10 years until they settled first in Fez (now in Morocco) and then Cairo. The family’s financial problems led Maimonides to train as a physician, and his skill led to a royal appointment within only a few years. He also worked as a rabbinic judge, but this was an activity for which he thought it wrong to accept any payment. He was recognized as head of the Jewish community of Cairo in 1191, and after his death his tomb became a place of Jewish pilgrimage.

Key works

1168 Commentary on the Mishna

1168–78 Mishneh Torah

1190 Guide of the Perplexed

## Epistemology

BEFORE

610 Islam is founded by the Prophet Mohammed.

644 Ali ibn Abi Talib, Mohammed’s cousin and successor, becomes Caliph.

10th century Ali’s mystical interpretation of the Qur’an becomes the basis for Sufism.

AFTER

1273 Rumi’s followers found the Mawlawi Order of Sufism.

1925 After the founding of a secular Republic of Turkey, the Mawlawi Order is banned in Turkey. It remains illegal until 1954, when it receives the right to perform on certain occasions.

Today Rumi’s works continue to be translated into many languages around the world.

Sufism, the mystical and aesthetic interpretation of the Qur’an, had been part of Islam since its foundation in the 7th century, but had not always been accepted by mainstream Islamic scholars. Jalal ad-Din Muhammad Rumi, better known simply as Rumi, was brought up in orthodox Islam, and first came into contact with Sufism when his family moved from the eastern edges of Persia to Anatolia in the mid-13th century. The Sufi concept of uniting with God through love caught his imagination, and from this he developed a version of Sufism that sought to explain the relationship of man with the divine.

Rumi became a teacher in a Sufi order, and as such he believed he was a medium between God and man. In contrast to general Islamic practice, he placed much emphasis on dhikr—ritual prayer or litany—rather than rational analysis of the Qur’an for divine guidance, and became known for his ecstatic revelations. He believed it was his task to communicate the visions he experienced, and so he wrote them down in the form of poetry. Central to his visionary philosophy is the idea that the universe and everything in it is an endless flow of life, in which God is an eternal presence. Man, as part of the universe, is also a part of this continuum, and Rumi seeks to explain our place within it.

Man, he believes, is a link between the past and future in a continual process of life, death, and rebirth—not as a cycle, but in a progression from one form to another stretching into eternity. Death and decay are inevitable and part of this endless flow of life, but as something ceases to exist in one form, it is reborn in another. Because of this, we should have no fear of death, and nor should we grieve a loss. In order to ensure our growth from one form to another, however, we should strive for spiritual growth and an understanding of the divine–human relationship. Rumi believes that this understanding comes from emotion rather than from reason—emotion enhanced by music, song, and dance.

The Mawlawi Order, or Whirling Dervishes, dance as part of the Sufi Sema ceremony. The dance represents the spiritual journey of man from ignorance to perfection through love.

Rumi’s legacy

The mystical elements of Rumi’s ideas were inspirational within Sufism, and influenced mainstream Islam too. They were also pivotal in converting much of Turkey from Orthodox Christianity to Islam. But this aspect of his thinking did not hold much sway in Europe, where rationalism was the order of the day. In the 20th century, however, his ideas became very popular in the West, mainly because his message of love chimed with the New Age values of the 1960s. Perhaps his greatest admirer in the 20th century was the poet and politician Muhammed Iqbal, advisor to Muhammad Ali Jinnah, who campaigned for an Islamic state of Pakistan in the 1930s.

"I died as a mineral and became a plant, I died as a plant and rose to animal, I died as animal and I was Man."

Jalal ad-Din Rumi

JALAL AD-DIN MUHAMMAD RUMI

Jalal ad-Din Muhammad Rumi, also known as Mawlana (Our Guide) or simply Rumi, was born in Balkh, in a province of Persia. When the Mongol invasions threatened the region, his family settled in Anatolia, Turkey, where Rumi met the Persian poets Attar and Shams al-Din Tabrizi. He decided to devote himself to Sufism, and went on to write thousands of verses of Persian and Arabic poetry.

In 1244 Rumi became the shaykh (Master) of a Sufi order, and taught his mystical-emotional interpretation of the Qur’an and the importance of music and dance in religious ceremony. After his death, his followers founded the Mawlawi Order of Sufism, which is famous for its Whirling Dervishes who perform a distinctive dance in the Sema ceremony—a form of dhikr unique to the sect.

Key works

Early–mid-13th century

Rhyming Couplets of Profound Spiritual Meaning

The Works of Shams of Tabriz

What is Within is Within

Seven Sessions

## Epistemology

BEFORE

380–360 BCE Plato writes on “the Good” or “the One” as the ultimate source of reason, knowledge, and all existence.

Late 5th century CE The Greek theologian and philosopher Dionysius the Areopagite describes God as “above being.”

c.860 Johannes Scotus Eriugena promotes the ideas of Dionysius the Areopagite.

AFTER

1492 Giovanni Pico della Mirandola’s On Being and the One marks a turning point in Renaissance thinking about God.

1991 French philosopher Jean-Luc Marion explores the theme of God as not a being.

Nikolaus von Kues belongs to a long tradition of medieval philosophers who attempt to describe the nature of God, stressing how God is unlike anything that the human mind is capable of grasping. Von Kues begins with the idea that we gain knowledge by using our reason to define things. So in order to know God, he deduces that we must try to define the basic nature of God.

Plato describes “the Good” or “the One” as the ultimate source of all other forms and knowledge, and some early Christian theologians talk of God as “above being.” Von Kues, writing around 1440, goes further, stating that God is what comes before everything, even before the possibility of something existing. Yet reason tells us the possibility of any phenomenon existing must come before its actual existence. It is impossible for something to come into being before the possibility of it arises. The conclusion that von Kues comes to, therefore, is that something that is said to do this must be described as “Not-other.”

"Whatever-I-know is not God and whatever-I-conceive is not like God."

Nikolaus von Kues

Beyond apprehension

However, the use of the word “thing” in the line of reasoning that von Kues adopts is misleading, as the “Not-other” has no substance. It is, according to von Kues, “beyond apprehension”, and is before all things in such a way that “they are not subsequent to it, but exist through it.” For this reason too, von Kues thinks “Not-other” comes closer to a definition of God than any other term.

## Epistemology

BEFORE

354–430 CE St. Augustine of Hippo integrates Platonism into Christianity.

c.1265–1274 Thomas Aquinas combines Aristotelian and Christian philosophy in his Summa Theologica.

AFTER

1517 Theologian Martin Luther writes The Ninety-Five Theses, protesting against clerical abuses. It triggers the start of the Reformation.

1637 René Descartes writes Discourse on the Method, putting human beings at the center of philosophy.

1689 John Locke argues for separation of government and religion in A Letter Concerning Toleration.

The treatise In Praise of Folly, which Erasmus wrote in 1509, reflects the Humanist ideas that were beginning to flood across Europe during the early years of the Renaissance, and were to play a key role in the Reformation. It is a witty satire on the corruption and doctrinal wranglings of the Catholic Church. However, it also has a serious message, stating that folly—by which Erasmus meant naive ignorance—is an essential part of being human, and is what ultimately brings us the most happiness and contentment. He goes on to claim that knowledge, on the other hand, can be a burden and can lead to complications that may make for a troublesome life.

Faith and folly

Religion is a form of folly too, Erasmus states, in that true belief can only ever be based on faith, never on reason. He dismisses the mixing of ancient Greek rationalism with Christian theology by medieval philosophers, such as St. Augustine of Hippo and Thomas Aquinas, as theological intellectualizing, claiming that it is the root cause of the corruption of religious faith. Instead, Erasmus advocates a return to simple heartfelt beliefs, with individuals forming a personal relationship with God, and not one prescribed by Catholic doctrine.

Erasmus advises us to embrace what he sees as the true spirit of the Scriptures—simplicity, naivety, and humility. These, he says, are the fundamental human traits that hold the key to a happy life.

"Happiness is reached when a person is ready to be what he is."

Desiderius Erasmus

## Epistemology

BEFORE

5th century BCE Plato argues in his Republic that the state should be governed by a philosopher-king.

1st century BCE The Roman writer Cicero argues that the Roman Republic is the best form of government.

AFTER

16th century Machiavelli’s peers begin to use the adjective “Machiavellian” to describe acts of devious cunning.

1762 Jean-Jacques Rousseau argues that people should hold on to their liberty and resist the rule of princes.

1928 Italian dictator Benito Mussolini describes The Prince as “the statesman’s supreme guide.”

In order fully to understand Machiavelli’s views on power, it is necessary to understand the background to his political concerns. Machiavelli was born in Florence, Italy, during a time of almost constant upheaval. The Medici family had been in open but unofficial control of the city-state for some 35 years, and the year of Machiavelli’s birth saw Lorenzo de’ Medici (Lorenzo the Magnificent) succeed his father as ruler, ushering in a period of great artistic activity in Florence. Lorenzo was succeeded in 1492 by his son Piero (known as Piero the Unfortunate), whose reign was short-lived. The French under Charles VIII invaded Italy in considerable force in 1494, and Piero was forced to surrender and then flee the city, as the citizens rebelled against him. Florence was declared a republic that same year.

The Dominican prior of the San Marco monastery, Girolamo Savonarola, then came to dominate Florentine political life. The city-state entered a democratic period under his guidance, but after accusing the pope of corruption Savonarola was eventually arrested and burnt as a heretic. This led to Machiavelli’s first known involvement in Florentine politics, and he became Secretary to the second Chancery in 1498.

Career and influences

The invasion by Charles VIII in 1494 had sparked a turbulent period in the history of Italy, which at the time was divided into five powers: the papacy, Naples, Venice, Milan, and Florence. The country was fought over by various foreign powers, mainly France, Spain, and the Holy Roman Empire. Florence was weak in the face of their armies, and Machiavelli spent 14 years travelling between various cities on diplomatic missions, trying to shore up the struggling republic.

In the course of his diplomatic activities, Machiavelli met Cesare Borgia, the illegitimate son of Pope Alexander VI. The pope was a powerful figure in northern Italy, and a significant threat to Florence. Although Cesare was Florence’s enemy, Machiavelli—despite his republican views—was impressed by his vigor, intelligence, and ability. Here we see one of the sources for Machiavelli’s famous work, The Prince.

Pope Alexander VI died in 1503, and his successor Pope Julius II was another strong and successful man who impressed Machiavelli with both his military ability and his cunning. But tension between France and the papacy led to Florence fighting with the French against the pope and his allies, the Spanish. The French lost, and Florence with them. In 1512 the Spanish dissolved the city-state’s government, the Medicis returned, and what was in effect a tyranny under Cardinal de’ Medici was installed. Machiavelli was fired from his political office and exiled to his farm in Florence. His political career might have revived under the rule of the Medicis, but in February 1513 he was falsely implicated in a plot against the family, and he was tortured, fined, and imprisoned.

Machiavelli was released from prison within a month, but his chances of re-employment were slim, and his attempts to find a new political position came to nothing. He decided to present the head of the de’ Medici family in Florence, Giuliano, with a book. By the time it was ready Giuliano had died, so Machiavelli changed the dedication to Giuliano’s successor, Lorenzo. The book was of a type popular at the time: advice to a prince.

Lorenzo the Magnificent (1449–1492) effectively ruled Florence from the death of his father in 1469 until his death. Though he ruled as a despot, the republic flourished under his guidance.

The Prince

Machiavelli’s book The Prince was witty and cynical, and showed a great understanding of Italy in general and Florence in particular. In it, Machiavelli sets out his argument that the goals of a ruler justify the means used to obtain them. The Prince differed markedly from other books of its type in its resolute setting aside of Christian morality. Machiavelli wanted to give ruthlessly practical advice to a prince and, as his experience with extremely successful popes and cardinals had shown him, Christian values should be cast aside if they got in the way.

Machiavelli’s approach centers on the notion of virtù, but this is not the modern notion of moral virtue. It shares more similarities with the medieval notion of virtues as the powers or functions of things, such as the healing powers of plants or minerals. Machiavelli is writing about the virtues of princes, and these were the powers and functions that concerned rule. The Latin root of virtù also relates it to manliness (as in “virile”), and this feeds into what Machiavelli has to say in its application both to the prince himself and to the state—where sometimes virtù is used to mean “success”, and describes a state that is to be admired and imitated.

Part of Machiavelli’s point is that a ruler cannot be bound by morality, but must do what it takes to secure his own glory and the success of the state over which he rules—an approach that became known as realism. But Machiavelli does not argue that the end justifies the means in all cases. There are certain means that a wise prince must avoid, for though they might achieve the desired ends, they lay him open to future dangers.

The main means to be avoided consist of those that would make the people hate their prince. They may love him, they may fear him—preferably both, Machiavelli says, though it is more important for a prince to be feared than to be loved. But the people must not hate him, for this is likely to lead to rebellion. Also, a prince who mistreats his people unnecessarily will be despised—a prince should have a reputation for compassion, not for cruelty. This might involve harsh punishment of a few in order to achieve general social order, which benefits more people in the long run.

In cases where Machiavelli does think that the end justifies the means, this rule applies only to princes. The proper conduct of citizens of the state is not at all the same as that of the prince. But even for ordinary citizens, Machiavelli generally disdains conventional Christian morality as being weak and unsuitable for a strong city.

"How difficult it is for a people accustomed to live under a prince to preserve their liberty!"

Niccolò Machiavelli

A ruler needs to know how to act like a beast, Machiavelli says in The Prince, and must imitate the qualities of the fox as well as the lion.

Prince or republic

There are reasons to suspect that The Prince does not represent Machiavelli’s own views. Perhaps the most important is the disparity between the ideas it contains and those expressed in his other main work, Discourses on the Ten Books of Titus Livy. In the Discourses Machiavelli argues that a republic is the ideal regime, and that it should be instituted whenever a reasonable degree of equality exists or can be established. A princedom is only suitable when equality does not exist in a state, and cannot be introduced. However, it can be argued that The Prince represents Machiavelli’s genuine ideas about how the ruler should rule in such cases; if princedoms are sometimes a necessary evil, it is best that they be ruled as well as possible. Moreover, Machiavelli did believe that Florence was in such political turmoil that it needed a strong ruler to get it into shape.

"It must be understood that a prince cannot observe all those things which are considered good in men."

Niccolò Machiavelli

Pleasing the readers

The fact that The Prince was written by Machiavelli in order to ingratiate himself with the Medicis is another reason to treat its contents with caution. However, he also dedicated the Discourses to members of Florence’s republican government. Machiavelli, it could be argued, would have written what the dedicatee wanted to read.

The Prince, however, contains much that Machiavelli is thought to have genuinely believed, such as the need for a citizens’ militia rather than reliance on mercenaries. The problem lies in discerning which parts are his actual beliefs and which are not. It is tempting to divide them according to how well they fit with the intended reader’s own beliefs, but that is unlikely to give an accurate result.

It has also been suggested that Machiavelli was attempting satire, and his real intended audience was the republicans, not the ruling elite. This idea is supported by the fact that Machiavelli did not write it in Latin, the language of the elite, but in Italian, the language of the people. Certainly, The Prince at times reads satirically, as though the audience is expected to conclude: “if that is how a good prince should behave, we should at all costs avoid being ruled by one!” If Machiavelli was also satirizing the idea that “the end justifies the means”, then the purpose of this small, deceptively simple book is far more intriguing than one might originally assume.

"The world has become more like that of Machiavelli."

Bertrand Russell

Ruthlessness has been a virtue of leadership throughout history. In the 20th century, the fascist dictator Benito Mussolini used a mixture of fear and love to hold on to power in Italy.

NICCOLÒ MACHIAVELLI

Machiavelli was born in Florence in 1469. Little is known of the first 28 years of his life; apart from a few inconclusive mentions in his father’s diary, the first direct evidence is a business letter written in 1497. From his writings, though, it is clear that he received a good education, perhaps at the University of Florence.

By 1498, Machiavelli had become a politician and diplomat of the Florentine Republic. After his enforced retirement on the return of the Medicis to Florence in 1512, he devoted himself to various literary activities, as well as persistent attempts to return to the political arena. Eventually he regained the trust of the Medicis, and Cardinal Giulio de’ Medici commissioned him to write a history of Florence. The book was finished in 1525, after the cardinal had become Pope Clement VII. Machiavelli died in 1527, without achieving his ambition to return to public life.

Key works

1513 The Prince

1517 Discourses on the Ten Books of Titus Livy

## Epistemology

BEFORE

4th century BCE Aristotle, in his Nicomachean Ethics, argues that to be virtuous, a person must be sociable and form close relationships with others; only a bestial man or a god can flourish alone.

AFTER

Late 18th century Anglican evangelical clergyman Richard Cecil states, “Solitude shows us what we should be; society shows us what we are.”

Late 19th century Friedrich Nietzsche describes solitude as necessary to the task of self-examination, which he claims can alone free humans from the temptation just to thoughtlessly follow the mob.

In his essay “On Solitude” (from the first volume of his Essays), Montaigne takes up a theme that has been popular since ancient times: the intellectual and moral dangers of living among others, and the value of solitude. Montaigne is not stressing the importance of physical solitude, but rather of developing the ability to resist the temptation to mindlessly fall in with the opinion and actions of the mob. He compares our desire for the approval of our fellow humans to being overly attached to material wealth and possessions. Both passions diminish us, Montaigne claims, but he does not conclude that we should relinquish either, only that we should cultivate a detachment from them. By doing so, we may enjoy them—and even benefit from them—but we will not become emotionally enslaved to them, or devastated if we lose them.

“On Solitude” then considers how our desire for mass approval is linked to the pursuit of glory, or fame. Contrary to thinkers such as Niccolò Machiavelli, who see glory as a worthy goal, Montaigne believes that constant striving for fame is the greatest barrier to peace of mind, or tranquility. He says of those who present glory as a desirable goal that they “only have their arms and legs out of the crowd; their souls, their wills, are more engaged with it than ever.”

Montaigne is not concerned with whether or not we achieve glory. His point is that we should shake off the desire for glory in the eyes of other people—that we should not always think of other people’s approval and admiration as being valuable. He goes on to recommend that instead of looking for the approbation of those around us, we should imagine that some truly great and noble being is constantly with us, able to observe our most private thoughts, a being in whose presence even the mad would hide their failings. By doing this, we will learn to think clearly and objectively and behave in a more thoughtful and rational manner. Montaigne claims that caring too much about the opinion of those around us will corrupt us, either because we end up imitating those who are evil, or become so consumed by hatred for them that we lose our reason.

Montaigne experienced the results of mindless mob violence during the French Wars of Religion (1562–98), including the atrocities of the St. Bartholomew Day Massacre of 1572.

Glory’s pitfalls

Montaigne returns to his attack on the pursuit of glory in his later writings, pointing out that the acquisition of glory is often so much a matter of mere chance that it makes little sense to hold it in such reverence. “Many times I’ve seen [fortune] stepping out ahead of merit, and often a long way ahead,” he writes. He also points out that encouraging statesmen and political leaders to value glory above all things, as Machiavelli does, merely teaches them never to attempt any endeavor unless an approving audience is on hand, ready and eager to bear witness to the remarkable nature of their powers and achievements.

"Contagion is very dangerous in crowds. You must either imitate the vicious or hate them."

Michel de Montaigne

MICHEL DE MONTAIGNE

Michel Eyquem de Montaigne was born and brought up in his wealthy family’s chateau near Bordeaux. However, he was sent to live with a poor peasant family until the age of three, so that he would be familiar with the life led by the ordinary workers. He received all his education at home, and was allowed to speak only Latin until the age of six. French was effectively his second language.

From 1557, Montaigne spent 13 years as a member of his local parliament, but resigned in 1571, on inheriting the family estates.

Montaigne published his first volume of Essays in 1580, going on to write two more volumes before his death in 1592. In 1580, he also set out on an extensive tour of Europe, partly to seek a cure for kidney stones. He returned to politics in 1581, when he was elected Mayor of Bordeaux, an office he held until 1585.

Key works

1569 In Defence of Raymond Sebond

1580–1581 Travel Journal

1580, 1588, 1595 Essays (3 volumes)

## Epistemology

BEFORE

4th century BCE Aristotle sets observation and inductive reasoning at the center of scientific thinking.

13th century English scholars Robert Grosseteste and Roger Bacon add experimentation to Aristotle’s inductive approach to scientific knowledge.

AFTER

1739 David Hume’s Treatise of Human Nature argues against the rationality of inductive thinking.

1843 John Stuart Mill’s System of Logic outlines the five inductive principles that together regulate the sciences.

1934 Karl Popper states that falsification, not induction, defines the scientific method.

Bacon is often credited with being the first in a tradition of thought known as British empiricism, which is characterized by the view that all knowledge must come ultimately from sensory experience. He was born at a time when there was a shift from the Renaissance preoccupation with the rediscovered achievements of the ancient world toward a more scientific approach to knowledge. There had already been some innovative work by Renaissance scientists such as the astronomer Nicolaus Copernicus and the anatomist Andreas Vesalius, but this new period—sometimes called the Scientific Revolution—produced an astonishing number of scientific thinkers, including Galileo Galilei, William Harvey, Robert Boyle, Robert Hooke, and Isaac Newton.

Although the Church had been broadly welcoming to science for much of the medieval period, this was halted by the rise of opposition to the Vatican’s authority during the Renaissance. Several religious reformers, such as Martin Luther, had complained that the Church had been too lax in countering scientific challenges to accounts of the world based on the Bible. In response, the Catholic Church, which had already lost adherents to Luther’s new form of Christianity, changed its stance and turned against scientific endeavor. This opposition, from both sides of the religious divide, hampered the development of the sciences.

Bacon claims to accept the teachings of the Christian Church. But he also argues that science must be separated from religion, in order to make the acquisition of knowledge quicker and easier, so that it can be used to improve the quality of people’s lives. Bacon stresses this transforming role for science. One of his complaints is that science’s ability to enhance human existence had previously been ignored, in favor of a focus on academic and personal glory.

Bacon presents a list of the psychological barriers to pursuing scientific knowledge in terms that he calls collectively the “idols of the mind.” These are the “idols of the tribe”, the tendency of human beings as a species (or “tribe”) to generalize; the “idols of the cave”, the human tendency to impose preconceptions on nature rather than to see what is really there; the “idols of the marketplace”, our tendency to let social conventions distort our experience; and the “idols of the theater”, the distorting influence of prevailing philosophical and scientific dogma. The scientist, according to Bacon, must battle against all these handicaps to gain knowledge of the world.

Science, not religion, was regarded increasingly as the key to knowledge from the 16th century onward. This 1598 print depicts the observatory of Danish astronomer Tycho Brahe (1546–1601).

Scientific method

Bacon goes on to argue that the advancement of science depends on formulating laws of ever-increasing generality. He proposes a scientific method that includes a variation of this approach. Instead of making a series of observations, such as instances of metals that expand when heated, and then concluding that heat must cause all metals to expand, he stresses the need to test a new theory by going on to look for negative instances—such as metals not expanding when they are heated.

Bacon’s influence led to a focus on practical experimentation in science. He was, however, criticized for neglecting the importance of the imaginative leaps that drive all scientific progress.

"By far the best proof is experience."

Francis Bacon

FRANCIS BACON

Born in London, Francis Bacon was educated privately, before being sent to Trinity College, Cambridge, at the age of 12. After graduation, he started training as a lawyer, but abandoned his studies to take up a diplomatic post in France. His father’s death in 1579 left him impoverished, forcing him to return to the legal profession.

Bacon was elected to parliament in 1584, but his friendship with the treasonous Earl of Essex held back his political career until the accession of James I in 1603. In 1618, he was appointed Lord Chancellor, but was dismissed two years later, when he was convicted of accepting bribes.

Bacon spent the rest of his life writing and carrying out his scientific work. He died from bronchitis, contracted while stuffing a chicken with snow, as part of an experiment in food preservation.

Key works

1597 Essays

1605 The Advancement of Learning

1620 Novum Organum

1624 Nova Atlantis

## Epistemology

BEFORE

4th century BCE Aristotle argues that whenever we perform any action, including thinking, we are conscious that we perform it, and in this way we are conscious that we exist.

c.420 CE St. Augustine writes in The City of God that he is certain he exists, because if he is mistaken, this itself proves his existence—in order to be mistaken, one must exist.

AFTER

1781 In his Critique of Pure Reason, Immanuel Kant argues against Descartes, but adopts the First Certainty—“I think therefore I exist”—as the heart and starting point of his idealist philosophy.

René Descartes lived in the early 17th century, during a period sometimes called the Scientific Revolution, an era of rapid advances in the sciences. The British scientist and philosopher Francis Bacon had established a new method for conducting scientific experiments, based on detailed observations and deductive reasoning, and his methodologies had provided a new framework for investigating the world. Descartes shared his excitement and optimism, but for different reasons. Bacon considered the practical applications of scientific discoveries to be their whole purpose and point, whereas Descartes was more fascinated by the project of extending knowledge and understanding of the world.

During the Renaissance—the preceding historical era—people had become more skeptical about science and the possibility of genuine knowledge in general, and this view continued to exert an influence in Descartes’ time. So a major motivation of his “project of pure enquiry”, as his work has become known, was the desire to rid the sciences of the annoyance of skepticism once and for all.

In the Meditations on First Philosophy, Descartes’ most accomplished and rigorous work on metaphysics (the study of being and reality) and epistemology (the study of the nature and limits of knowledge), he seeks to demonstrate the possibility of knowledge even from the most skeptical of positions, and from this, to establish a firm foundation for the sciences. The Meditations is written in the first-person form—“I think…”—because he is not presenting arguments in order to prove or disprove certain statements, but instead wishes to lead the reader along the path that he himself has taken. In this way the reader is forced to adopt the standpoint of the meditator, thinking things through and discovering the truth just as Descartes had done. This approach is reminiscent of the Socratic method, in which the philosopher gradually draws out a person’s understanding rather than presenting it already packaged and ready to take away.

Descartes’ book De Homine Figuris takes a biological look at the causes of knowledge. In it, he suggests that the pineal gland is the link between vision and conscious action.

The illusory world

In order to establish that his beliefs have stability and endurance, which Descartes takes to be two important marks of knowledge, he uses what is known as “the method of doubt.” This starts with the meditator setting aside any belief whose truth can be doubted, whether slightly or completely. Descartes’ aim is to show that, even if we start from the strongest possible skeptical position, doubting everything, we can still reach knowledge. The doubt is “hyperbolic” (exaggerated), and used only as a philosophical tool; as Descartes points out: “no sane person has ever seriously doubted these things.”

Descartes starts by subjecting his beliefs to a series of increasingly rigorous skeptical arguments, questioning how we can be sure of the existence of anything at all. Could it be that the world we know is just an illusion? We cannot trust our senses, as we have all been “deceived” by them at one time or another, and so we cannot rely on them as a sure footing for knowledge. Perhaps, he says, we are dreaming, and the apparently real world is no more than a dream world. He notes that this is possible, as there are no sure signs between being awake or asleep. But even so, this situation would leave open the possibility that some truths, such as mathematical axioms, could be known, though not through the senses. But even these “truths” might not in fact be true, because God, who is all-powerful, could deceive us even at this level. Even though we believe that God is good, it is possible that he made us in such a way that we are prone to errors in our reasoning. Or perhaps there is no God—in which case we are even more likely to be imperfect beings (having arisen only by chance) that are capable of being deceived all the time.

Having reached a position in which there seems to be nothing at all of which he can be certain, Descartes then devises a vivid tool to help him to avoid slipping back into preconceived opinion: he supposes that there is a powerful and evil demon who can deceive him about anything. When he finds himself considering a belief, he can ask: “Could the demon be making me believe this even though it was false?” and if the answer is “yes” he must set aside the belief as open to doubt.

At this point, it seems as though Descartes has put himself into an impossible position—nothing seems beyond doubt, so he has no solid ground on which to stand. He describes himself as feeling helplessly tumbled around by a whirlpool of universal doubt, unable to find his footing. Skepticism seems to have made it impossible for him even to begin his journey back to knowledge and truth.

"It is necessary that at least once in your life you doubt, as far as possible, all things."

René Descartes

An optical illusion of parallel lines that are made to look bent can fool our senses. Descartes thinks we must accept nothing as true or given, but must instead strip away all preconceptions before we can proceed to a position of knowledge.

An evil demon capable of deceiving humankind about everything cannot make me doubt my existence; if he tries, and I am forced to question my own existence, this only confirms it.

The First Certainty

It is at this point that Descartes realizes that there is one belief that he surely cannot doubt: his belief in his own existence. Each of us can think or say: “I am, I exist”, and while we are thinking or saying it we cannot be wrong about it. When Descartes tries to apply the evil demon test to this belief, he realizes that the demon could only make him believe that he exists if he does in fact exist; how can he doubt his existence unless he exists in order to do the doubting?

This axiom—“I am, I exist” –forms Descartes’ First Certainty. In his earlier work, the Discourse on the Method, he presented it as: “I think therefore I am”, but he abandoned this wording when he wrote the Meditations, as the inclusion of “therefore” makes the statement read like a premise and conclusion. Descartes wants the reader—the meditating “I”—to realize that as soon as I consider the fact that I exist, I know it to be true. This truth is instantly grasped. The realization that I exist is a direct intuition, not the conclusion of an argument.

Despite Descartes’ move to a clearer expression of his position, the earlier formulation was so catchy that it stuck in people’s minds, and to this day the First Certainty is generally known as “the cogito”, from the Latin cogito ergo sum, meaning “I think therefore I am.” St. Augustine of Hippo had used a very similar argument in The City of God, when he said: “For if I am mistaken, I exist”; meaning that if he did not exist, he could not be mistaken. Augustine, however, made little use of this in his thinking, and certainly did not reach it in the way that Descartes did.

What use, though, is a single belief? The simplest logical argument is a syllogism, which has two premises and a conclusion—such as: all birds have wings; a robin is a bird; therefore all robins have wings. We surely cannot get anywhere from the starting point of just one true belief. But Descartes was not looking to reach these kinds of conclusions from his First Certainty. As he explained: “Archimedes used to demand just one firm and immovable point in order to shift the entire Earth.” For Descartes, the certainty of his own existence gives him the equivalent; it saves him from that whirlpool of doubt, gives him a firm foothold, and so allows him to start on the journey back from skepticism to knowledge. It is crucial to his project of enquiry, but it is not the foundation of his epistemology.

"I shall suppose that some malicious demon of the utmost power and cunning has employed all his energies in order to deceive me."

René Descartes

What is this “I”?

Despite the fact that the First Certainty’s main function is to provide a firm footing for knowledge, Descartes realizes that we might also be able to gain knowledge from the certainty itself. This is because the knowledge that I am thinking is bound up with the knowledge of my existence. So “thinking” is also something that I cannot rationally doubt, for doubting is a kind of thinking, so to doubt that I am thinking is to be thinking. As Descartes now knows that he exists and that he is thinking, then he—and every other meditator—also knows that he is a thinking thing.

Descartes makes clear, though, that this is as far as he can reason from the First Certainty. He is certainly not entitled to say that he is only a thinking thing—a mind—as he has no way of knowing what more he might be. He might be a physical thing that also has the ability to think, or he might be something else, something that he has not even conceived yet. The point is that at this stage of his meditations he knows only that he is a thinking thing; as he puts it, he knows only that he is, “in the strict sense only” a thinking thing. Later, in the sixth book of the Meditations, Descartes presents an argument that mind and body are different sorts of thing—that they are distinct substances—but he is not yet in a position to do so.

The only question that Descartes is definitely able to answer using his method of doubt is whether he is thinking. He cannot prove the existence of his body or of the external world.

"This proposition, I am, I exist, is necessarily true whenever it is put forward by me or conceived in my mind."

René Descartes

Doubting Descartes

This First Certainty has been the target of criticism from many writers who hold that Descartes’ approach to skepticism is doomed from the start. One of the main arguments against it takes issue with the very use of the term “I” in “I am, I exist.” Although Descartes cannot be wrong in saying that thinking is occurring, how does he know that there is “a thinker”—a single, unified consciousness doing that thinking? What gives him the right to assert the existence of anything beyond the thoughts? On the other hand, can we make sense of the notion of thoughts floating around without a thinker?

It is difficult to imagine detached, coherent thoughts, and Descartes argues that it is impossible to conceive of such a state of affairs. However, if one were to disagree, and believe that a world of thoughts with no thinkers is genuinely possible, Descartes would not be entitled to the belief that he exists, and would thus fail to reach his First Certainty. The existence of thoughts would not give him the solid ground he needed.

The problem with this notion of thoughts floating around with no thinker is that reasoning would be impossible. In order to reason, it is necessary to relate ideas in a particular way. For example, if Patrick has the thought “all men are mortal” and Patricia has the thought “Socrates is a man”, neither can conclude anything. But if Paula has both thoughts, she can conclude that “Socrates is mortal.” Merely having the thoughts “all men are mortal” and “Socrates is a man” floating around is like two separate people having them; in order for reason to be possible we need to make these thoughts relative to one another, to link them in the right way. It turns out that making thoughts relative to anything other than a thinker (for example, to a place or to a time) fails to do the job. And since reasoning is possible, Descartes can conclude that there is a thinker.

Some modern philosophers have denied that Descartes’ certainty of his own existence can do the job he requires of it; they argue that “I exist” has no content, as it merely refers to its subject but says nothing meaningful or important about it; it is simply pointing at the subject. For this reason nothing can follow from it, and Descartes’ project fails at the beginning. This seems to miss Descartes’ point; as we have seen, he does not use the First Certainty as a premise from which to derive further knowledge—all he needs is that there be a self for him to point to. So even if “I exist” only succeeds in pointing to the meditator, then he has an escape from the whirlpool of doubt.

"When someone says ‘I am thinking, therefore I am’, he recognizes it as something self-evident by a simple intuition of the mind."

René Descartes

An unreal thinker

For those who have misunderstood Descartes to have been offering an argument from the fact of his thinking to the fact of his existence, we can point out that the First Certainty is a direct intuition, not a logical argument. Why, though, would it be a problem if Descartes had been offering an argument?

As it stands, the apparent inference “I am thinking, therefore I exist” is missing a major premise; that is, in order for the argument to work it needs another premise, such as “anything that is thinking exists.” Sometimes an obvious premise is not actually stated in an argument, in which case it is known as a suppressed premise. But some of Descartes’ critics complain that this suppressed premise is not at all obvious. For example, Hamlet, in Shakespeare’s play, thought a great deal, but it is also clearly true that he did not exist; so it is not true that anything that thinks exists.

We might say that in so far as Hamlet thought, he thought in the fictional world of a play, but he also existed in that fictional world; in so far as he did not exist, he did not exist in the real world. His “reality” and thinking are linked to the same world. But Descartes’ critics might respond that that is precisely the point: knowing that someone called Hamlet was thinking—and no more than this—does not assure us that this person exists in the real world; for that, we should have to know that he was thinking in the real world. Knowing that something or someone—like Descartes—is thinking, is not enough to prove their reality in this world.

The answer to this dilemma lies in the first-person nature of the Meditations, and the reasons for Descartes’ use of the “I” throughout now becomes clear. Because while I might be unsure whether Hamlet was thinking, and therefore existed, in a fictional world or the real world, I cannot be unsure about myself.

"We ought to enquire as to what sort of knowledge human reason is capable of attaining, before we set about acquiring knowledge of things in particular."

René Descartes

Modern philosophy

In the “Preface to the Reader” of the Meditations, Descartes accurately predicted that many readers would approach his work in such a way that most would “not bother to grasp the proper order of my arguments and the connection between them, but merely try to carp at individual sentences, as is the fashion.” On the other hand, he also wrote that “I do not expect any popular approval, or indeed any wide audience”, and in this he was much mistaken. He is often described as the father of modern philosophy. He sought to give philosophy the certainty of mathematics without recourse to any kind of dogma or authority, and to establish a firm, rational foundation for knowledge. He is also well known for proposing that the mind and the body are two distinct substances—one material (the body) and the other immaterial (the mind)—which are nonetheless capable of interaction. This famous distinction, which he explains in the Sixth Meditation, became known as Cartesian dualism.

However, it is the rigor of Descartes’ thought and his rejection of any reliance on authority that are perhaps his most important legacy. The centuries after his death were dominated by philosophers who either developed his ideas or those who took as their main task the refutation of his thoughts, such as Thomas Hobbes, Benedictus Spinoza, and Gottfried Leibniz.

The separation of mind and body theorized by Descartes leaves open the following question: since all we can see of ourselves is our bodies, how could we prove that a robot is not conscious?

RENÉ DESCARTES

René Descartes was born near Tours, France, and was educated at the Jesuit Collège Royale, in La Flèche. Due to ill-health, he was allowed to stay in bed until late in the mornings, and he formed the habit of meditating. From the age of 16 he concentrated on studying mathematics, breaking off his studies for four years to volunteer as a soldier in Europe’s Thirty Years War. During this time he found his philosophical calling, and after leaving the army, he settled first in Paris and then in the Netherlands, where he spent most of the rest of his life. In 1649 he was invited to Sweden by Queen Christina to discuss philosophy; he was expected to get up very early, much against his normal practice. He believed that this new regime—and the Swedish climate—caused him to contract pneumonia, of which he died a year later.

Key works

1637 Discourse on the Method

1641 Meditations on First Philosophy

1644 Principles of Philosophy

1662 De Homine Fuguris

## Epistemology

BEFORE

c.350 BCE Aristotle says that “imagination is the process by which we say that an image is presented to us,” and that “the soul never thinks without a mental image.”

1641 René Descartes claims that the philosopher must train his imagination for the sake of gaining knowledge.

AFTER

1740 In his Treatise of Human Nature, David Hume argues that “nothing we imagine is absolutely impossible.”

1787 Immanuel Kant claims that we synthesize the incoherent messages from our senses into images, and then into concepts, using the imagination.

Pascal’s best-known book, Pensées, is not primarily a philosophical work. Rather, it is a compilation of fragments from his notes for a projected book on Christian theology. His ideas were aimed primarily at what he called libertins—ex-Catholics who had left religion as a result of the sort of free thinking encouraged by skeptical writers such as Montaigne. In one of the longer fragments, Pascal discusses imagination. He offers little or no argument for his claims, being concerned merely to set down his thoughts on the matter.

Pascal’s point is that imagination is the most powerful force in human beings, and one of our chief sources of error. Imagination, he says, causes us to trust people despite what reason tells us. For example, because lawyers and doctors dress up in special clothes, we tend to trust them more. Conversely, we pay less attention to someone who looks shabby or odd, even if he is talking good sense.

What makes things worse is that, though it usually leads to falsehood, imagination occasionally leads to truth; if it were always false, then we could use it as a source of certainty by simply accepting its negation.

After presenting the case against imagination in some detail, Pascal suddenly ends his discussion of it by writing: “Imagination decides everything: it produces beauty, justice, and happiness, which is the greatest thing in the world.” Out of context, it might seem that he is praising imagination, but we can see from what preceded this passage that his intention is very different. As imagination usually leads to error, then the beauty, justice, and happiness that it produces will usually be false. In the wider context of a work of Christian theology, and especially in light of Pascal’s emphasis on the use of reason to bring people to religious belief, we can see that his aim is to show the libertins that the life of pleasure that they have chosen is not what they think it is. Although they believe that they have chosen the path of reason, they have in fact been misled by the power of the imagination.

"Man is but a reed, the weakest nature; yet he is a thinking reed."

Blaise Pascal

Pascal’s Wager

This view is relevant to one of the most complete notes in the Pensées, the famous argument known as Pascal’s Wager. The wager was designed to give the libertins a reason to return to the Church, and it is a good example of “voluntarism”, the idea that belief is a matter of decision. Pascal accepts that it is not possible to give good rational grounds for religious belief, but tries to offer rational grounds for wanting to have such beliefs. These consist of weighing up the possible profit and loss of making a bet on the existence of God. Pascal argues that betting that God does not exist risks losing a great deal (infinite happiness in Heaven), while only gaining a little (a finite sense of independence in this world)—but betting that God exists risks little while gaining a great deal. It is more rational, on this basis, to believe in God.

According to Pascal, we are constantly tricked by the imagination into making the wrong judgments—including judgements about people based on how they are dressed.

BLAISE PASCAL

Blaise Pascal was born in Clermont-Ferrand, France. He was the son of a government functionary who had a keen interest in science and mathematics and who educated Pascal and his two sisters. Pascal published his first mathematical paper at the age of 16, and had invented the first digital calculator by the time he was 18. He also corresponded with the famous mathematician Pierre Fermat, with whom he laid the foundations of probability theory.

Pascal underwent two religious conversions, first to Jansenism (an approach to Christian teaching that was later declared heretical), and then to Christianity proper. This led him to abandon his mathematical and scientific work in favor of religious writings, including the Pensées. In 1660–62 he instituted the world’s first public transport service, giving all profits to the poor, despite suffering from severe ill health from the 1650s until his death in 1662.

Key works

1657 Lettres Provinciales

1670 Pensées

## Empiricism

BEFORE

c.380 BCE In his dialogue, Meno, Plato argues that we remember knowledge from previous lives.

Mid-13th century Thomas Aquinas puts forward the principle that “whatever is in our intellect must have previously been in the senses.”

AFTER

Late 17th century Gottfried Leibniz argues that the mind may seem to be a tabula rasa at birth, but contains innate, underlying knowledge, which experience gradually uncovers.

1966 Noam Chomsky, in Cartesian Linguistics, sets out his theory of innate grammar.

John Locke is traditionally included in the group of philosophers known as the British Empiricists, together with two later philosophers, George Berkeley and David Hume. The empiricists are generally thought to hold the view that all human knowledge must come directly or indirectly from the experience of the world that we acquire through the use of our senses alone. This contrasts with the thinking of the rationalist philosophers, such as René Descartes, Benedictus Spinoza, and Gottfried Leibniz, who hold that in principle, at least, it is possible to acquire knowledge solely through the use of reason.

In fact, the division between these two groups is not as clear-cut as is often assumed. The rationalists all accept that in practice our knowledge of the world ultimately stems from our experience, and most notably from scientific enquiry. Locke reaches his distinctive views concerning the nature of the world by applying a process of reasoning later known as abduction (inference to the best explanation from the available evidence) to the facts of sensory experience. For example, Locke sets out to demonstrate that the best explanation of the world as we experience it is corpuscular theory. This is the theory that everything in the world is made up of submicroscopic particles, or corpuscles, which we can have no direct knowledge of, but which, by their very existence, make sense of phenomena that would otherwise be difficult or impossible to explain. Corpuscular theory was becoming popular in 17th-century scientific thinking and is fundamental to Locke’s view of the physical world.

Innate ideas

The claim that man’s knowledge cannot go beyond his experience may therefore seem inappropriate, or at least an exaggeration, when attributed to Locke. However, Locke does argue at some length, in his Essay Concerning Human Understanding, against the theory proposed by the rationalists to explain how knowledge could be accessed without experience. This is the theory of innate ideas.

The concept that human beings are born with innate ideas, and that these can give us knowledge about the nature of the world around us, independently of anything we may experience, dates back to the dawn of philosophy. Plato had developed a concept, according to which all genuine knowledge is essentially located within us, but that when we die our souls are reincarnated into new bodies and the shock of birth causes us to forget it all. Education is therefore not about learning new facts, but about “unforgetting”, and the educator is not a teacher but a midwife.

However, many later thinkers countered Plato’s theory, proposing that all knowledge cannot be innate and that only a limited number of concepts can be. These include the concept of God and also that of a perfect geometric structure, such as an equilateral triangle. This type of knowledge, in their view, can be gained without any direct sensory experience, in the way that it is possible to devise a mathematical formula by using nothing more than the powers of reason and logic. René Descartes, for example, declares that although he believes that we all have an idea of God imprinted in us—like the mark that a craftsman makes in the clay of a pot—this knowledge of God’s existence can only be brought into our conscious mind through a process of reasoning.

"If we attentively consider newborn children, we shall have little reason to think that they bring many ideas into the world with them."

John Locke

Locke’s objections

Locke was against the idea that human beings possess any kind of innate knowledge. He takes the view that the mind at birth is a tabula rasa—a blank tablet or a new sheet of paper upon which experience writes, in the same way that light can create images on photographic film. According to Locke, we bring nothing to the process except the basic human ability to apply reason to the information that we gather through our senses. He argues that there is not the slightest empirical evidence to suggest that the minds of infants are other than blank at birth, and adds that this is also true of the minds of the mentally deficient, stating that “they have not the least apprehension or thought of them.” Locke, therefore, declares that any doctrine supporting the existence of innate ideas must be false.

Locke also goes on to attack the very notion of innate ideas by arguing that it is incoherent. In order for something to be an idea at all, he states that it has to have been present at some point in somebody’s mind. But, as Locke points out, any idea that claims to be truly innate must also be claiming to precede any form of human experience. Locke accepts that it is true, as Gottfried Leibniz states, that an idea may exist so deep in a person’s memory that for a time it is difficult or even impossible to recall, and so is not accessible to the conscious mind. Innate ideas, on the other hand, are believed to somehow exist somewhere, before the presence of any sort of mechanism that is capable of conceiving them and bringing them into consciousness.

The supporters of the existence of innate ideas often also argue that as such ideas are present in all human beings at birth, they must be by nature universal, which means that they are found in all human societies at all points in history. Plato, for example, claims that everyone potentially has access to the same basic body of knowledge, denying any difference in that respect between men and women, or between slaves and freemen. Similarly, in Locke’s time, the theory was frequently put forward that because innate ideas can only be placed in us by God, they must be universal, as God is not capable of being so unfair as to hand them out only to a select group of people.

Locke counters the argument for universal ideas by once again bringing to our attention that a simple examination of the world around us will readily show that they do not exist. Even if there were concepts, or ideas, which absolutely every human being in the world held in common, Locke argues that we would have no firm grounds for concluding that they were also innate. He declares that it would always be possible to discover other explanations for their universality, such as the fact that they stem from the most basic ways in which a human being experiences the world around him, which is something that we all must share.

In 1704, Gottfried Leibniz wrote a rebuttal of Locke’s empiricist arguments in his New Essays on the Human Understanding. Leibniz declares that innate ideas are the one clear way that we can gain knowledge that is not based upon sensory experience, and that Locke is wrong to deny their possibility. The debate about whether human beings can know anything beyond what they perceive through their five basic senses continues.

Locke believed the human mind is like a blank canvas, or tabula rasa, at birth. He states that all our knowledge of the world can only come from our experience, conveyed to us by our senses. We can then rationalize this knowledge to formulate new ideas.

"It seems to me a near contradiction to say that there are truths imprinted on the soul, which it perceives or understands not."

John Locke

Language as innate

Although Locke may reject the doctrine of innate ideas, he does not reject the concept that human beings have innate capacities. Indeed, the possession of capacities such as perception and reasoning are central to his accounts of the mechanism of human knowledge and understanding. In the late 20th century, the American philosophy Noam Chomsky took this idea further when he put forward his theory that there is an innate process of thinking in every human mind, which is capable of generating a universal “deep structure” of language. Chomsky believes that regardless of their apparent structural differences, all human languages have been generated from this common basis.

Locke played an important role in questioning how human beings acquire knowledge, at a time when man’s understanding of the world was expanding at an unprecedented rate. Earlier philosophers—notably the medieval Scholastic thinkers such as Thomas Aquinas—had concluded that some aspects of reality were beyond the grasp of the human mind. But Locke took this a stage further. By detailed analysis of man’s mental faculties, he sought to set down the exact limits of what is knowable.

"Let us then suppose the mind to be white paper, void of all characters, without any ideas; how comes it to be furnished?"

John Locke

As the mind is a blank canvas, or tabula rasa, at birth, Locke believes that anybody can be transformed by a good education, one that encourages rational thought and individual talents.

JOHN LOCKE

John Locke was born in 1632, the son of an English country lawyer. Thanks to wealthy patrons, he received a good education, first at Westminster School in London, then at Oxford. He was impressed with the empirical approach to science adopted by the pioneering chemist Robert Boyle, and he both promoted Boyle’s ideas and assisted in his experimental work.

Though Locke’s empiricist ideas are important, it was his political writing that made him famous. He proposed a social-contract theory of the legitimacy of government and the idea of natural rights to private property. Locke fled England twice, as a political exile, but returned in 1688, after the accession to the throne of William and Mary. He remained in England, writing as well as holding various government positions, until his death in 1704.

Key works

1689 A Letter Concerning Toleration

1690 An Essay Concerning Human Understanding

1690 Two Treatises of Government

## Rationalism

BEFORE

1340 Nicolaus of Autrecourt argues that there are no necessary truths about the world, only contingent truths.

1600s René Descartes claims that ideas come to us in three ways; they can be derived from experience, drawn from reason, or known innately (being created in the mind by God).

AFTER

1748 David Hume explores the distinction between necessary and contingent truths.

1927 Alfred North Whitehead postulates “actual entities”, similar to Leibniz’s monads, which reflect the whole universe in themselves.

Early modern philosophy is often presented as being divided into two schools—that of the rationalists (including René Descartes, Benedictus Spinoza, and Immanuel Kant) and that of the empiricists (including John Locke, George Berkeley, and David Hume). In fact, the various philosophers did not easily fall into two clear groups, each being like and unlike each of the others in complex and overlapping ways. The essential difference between the two schools, however, was epistemological—that is, they differed in their opinions about what we can know, and how we know what we know. Put simply, the empiricists held that knowledge is derived from experience, while the rationalists claimed that knowledge can be gained through rational reflection alone.

Leibniz was a rationalist, and his distinction between truths of reasoning and truths of fact marks an interesting twist in the debate between rationalism and empiricism. His claim, which he makes in most famous work, the Monadology, is that in principle all knowledge can be accessed by rational reflection. However, due to shortcomings in our rational faculties, human beings must also rely on experience as a means of acquiring knowledge.

"We know hardly anything adequately, few things a priori, and most things through experience."

Gottfried Wilhelm Leibniz

A map of the internet shows the innumerable connections between internet users. Leibniz’s theory of monads suggests that all our minds are similarly connected.

A universe in our minds

To see how Leibniz arrives at this conclusion, we need to understand a little of his metaphysics—his view of how the universe is constructed. He holds that every part of the world, every individual thing, has a distinct concept or “notion” associated with it, and that every such notion contains within it everything that is true about itself, including its relations to other things. Because everything in the universe is connected, he argues, it follows that every notion is connected to every other notion, and so it is possible—at least in principle—to follow these connections and to discover truths about the entire universe through rational reflection alone. Such reflection leads to Leibniz’s “truths of reasoning.” However, the human mind can grasp only a small number of such truths (such as those of mathematics), and so it has to rely on experience, which yields “truths of fact.”

So how is it possible to progress from knowing that it is snowing, for example, to knowing what will happen tomorrow somewhere on the other side of the world? For Leibniz, the answer lies in the fact that the universe is composed of individual, simple substances called “monads.” Each monad is isolated from other monads, and each contains a complete representation of the whole universe in its past, present, and future states. This representation is synchronized between all the monads, so that each one has the same content. According to Leibniz, this is how God created things—in a state of “pre-established harmony.”

Leibniz claims that every human mind is a monad, and so contains a complete representation of the universe. It is therefore possible in principle for us to learn everything that there is to know about our world and beyond simply by exploring our own minds. Simply by analyzing my notion of the star Betelgeuse, for example, I will eventually be able to determine the temperature on the surface of the actual star Betelgeuse. However, in practice, the analysis that is required for me reach this information is impossibly complex—Leibniz calls it “infinite”—and because I cannot complete it, the only way that I can discover the temperature of Betelgeuse is by measuring it empirically using astronomical equipment.

Is the temperature of the surface of Betelgeuse a truth of reasoning or a truth of fact? It may be true that I had to resort to empirical methods to discover the answer, but had my rational faculties been better I could also have discovered it through rational reflection. Whether it is a truth of reasoning or a truth of fact, therefore, seems to depend on how I arrive at the answer—but is this what Leibniz is claiming?

"Each singular substance expresses the whole universe in its own way."

Gottfried Wilhelm Leibniz

Necessary truths

The trouble for Leibniz is that he holds that truths of reasoning are “necessary”, meaning that it is impossible to contradict them, while truths of fact are “contingent”; they can be denied without logical contradiction. A mathematical truth is a necessary truth, because denying its conclusions contradicts the meanings of its own terms. But the proposition “it is raining in Spain” is contingent, because denying it does not involve a contradiction in terms—although it may still be factually incorrect.

Leibniz’s distinction between truths of reasoning and truths of fact is not simply an epistemological one (about the limits of knowledge), but also a metaphysical one (about the nature of the world), and it is not clear that his arguments support his metaphysical claim. Leibniz’s theory of monads seems to suggest that all truths are truths of reasoning, which we would have access to if we could finish our rational analysis. But as a truth of reasoning is a necessary truth, in what way is it impossible for the temperature on Betelgeuse to be 2,401 Kelvin rather than 2,400 Kelvin? Certainly not impossible in the sense that the proposition 2 + 2 = 5 is impossible, for the latter is simply a logical contradiction.

Likewise, if we follow Leibniz and separate neccesary and contingent truths, we end up with the following problem: I can discover Pythagoras’s theorem simply by reflecting on the idea of triangles, so Pythagoras’s theorem must be a truth of reasoning. But Betelgeuse’s temperature and Pythagoras’s theorem are both just as true, and just as much part of the monad that is my mind—so why should one be considered contingent and the other necessary?

Moreover, Leibniz tells us that whereas no-one can reach the end of an infinite analysis, God can grasp the whole universe at once, and so for him all truths are neccessary truths. The difference between a truth of reasoning and a truth of fact, therefore, does seem to be a matter of how one comes to know it—and in that case it is difficult to see why the former should always be seen to be necessarily true, while the latter may or may not be true.

"God understands everything through eternal truth, since he does not need experience."

Gottfried Wilhelm Leibniz

An uncertain future

In setting out a scheme in which an omnipotent, omniscient God creates the universe, Leibniz inevitably faces the problem of accounting for the notion of freedom of will. How can I choose to act in a certain way if God already knows how I am going to act? But the problem runs deeper—there seems to be no room for genuine contingency at all. Leibniz’s theory only allows for a distinction between truths whose necessity we can discover, and truths whose necessity only God can see. We know (if we accept Leibniz’s theory) that the future of the world is set by an omniscient and benevolent god, who therefore has created the best of all possible worlds. But we call the future contingent, or undetermined, because as limited human beings we cannot see its content.

The mechanical calculator was one of Leibniz’s many inventions. Its creation is a testament to his interest in mathematics and logic—fields in which he was a great innovator.

Leibniz’s legacy

In spite of the difficulties inherent in Leibniz’s theory, his ideas went on to shape the work of numerous philosophers, including David Hume and Immanuel Kant. Kant refined Leibniz’s truths of reasoning and truths of fact into the distinction between “analytic” and “synthetic” statements—a division that has remained central to European philosophy ever since.

Liebniz’s theory of monads fared less well, and was criticized for its metaphysical extravagance. In the 20th century, however, the idea was rediscovered by scientists who were intrigued by Leibniz’s description of space and time as a system of relationships, rather than the absolutes of traditional Newtonian physics.

GOTTFRIED LEIBNIZ

Gottfried Leibniz was a German philosopher and mathematician. He was born in Leipzig, and after university he took public service with the Elector of Mainz for five years, during which time he concentrated mainly on political writings. After a period spent travelling, he took up the post of librarian to the Duke of Brunswick, in Hanover, and remained there until his death. It was during this last period of his life that he did most of the work on the development of his unique philosophical system.

Leibniz is famous in mathematics for his invention of the so-called “infinitesimal calculus” and the argument that followed this, as both Leibniz and Newton claimed the discovery as their own. It seems clear that they had in fact reached it independently, but Leibniz developed a much more usable notation which is still used today.

Key works

1673 A Philosopher’s Creed

1685 Discourse on Metaphysics

1695 The New System

1710 Theodicy

1714 Monadology

## Scepticism

BEFORE

350 BCE Aristotle makes the first reference to a child’s mind as a “blank slate”, which later became known as a tabula rasa.

1690s John Locke argues that sense experience allows both children and adults to acquire reliable knowledge about the external world.

AFTER

1859 John Stuart Mill argues against assuming our own infallibility in On Liberty.

1900s Hans-Georg Gadamer and the postmodernists apply sceptical reasoning to all forms of knowledge, even that gained through empirical (sense-based) information.

Voltaire was a French intellectual who lived in the Age of Enlightenment. This period was characterized by an intense questioning of the world and how people live in it. European philosophers and writers turned their attention to the acknowledged authorities—such as the Church and state—to question their validity and their ideas, while also searching for new perspectives. Until the 17th century, Europeans had largely accepted the Church’s explanations of what, why, and how things existed, but both scientists and philosophers had begun to demonstrate different approaches to establishing the truth. In 1690 the philosopher John Locke had argued that no ideas were innate (known at birth), and that all ideas arise from experience alone. His argument was given further weight by scientist Isaac Newton whose experiments provided new ways of discovering truths about the world. It was against this background of rebellion against the accepted traditions that Voltaire pronounced that certainty is absurd.

Voltaire refutes the idea of certainty in two ways. First, he points out that apart from a few necessary truths of mathematics and logic, nearly every fact and theory in history has been revised at some point in time. So what appears to be “fact” is actually little more than a working hypothesis. Second, he agrees with Locke that there is no such thing as an innate idea, and points out that ideas we seem to know as true from birth may be only cultural, as these change from country to country.

Revolutionary doubt

Voltaire does not assert that there are no absolute truths, but he sees no means of reaching them. For this reason he thinks doubt is the only logical standpoint. Given that endless disagreement is therefore inevitable, Voltaire says that it is important to develop a system, such as science, to establish agreement.

In claiming that certainty is more pleasant than doubt, Voltaire hints at how much easier it is simply to accept authoritative statements—such as those issued by the monarchy or Church—than it is to challenge them and think for yourself. But Voltaire believes it is vitally important to doubt every “fact” and to challenge all authority. He holds that government should be limited but speech uncensored, and that science and education lead to material and moral progress. These were fundamental ideals of both the Enlightenment and the French Revolution, which took place 11 years after Voltaire’s death.

Scientific experiments during the Age of Enlightenment seemed to Voltaire to lead the way toward a better world, based on empirical evidence and unabashed curiosity.

VOLTAIRE

Voltaire was the pseudonym of the French writer and thinker, François Marie Arouet. He was born into a middle-class family in Paris, and was the youngest of three children. He studied law at university, but always preferred writing, and by 1715 was famous as a great literary wit. His satirical writing often landed him in trouble: he was imprisoned several times for insulting nobility, and was once exiled from France. This led to a stay in England, where he fell under the influence of English philosophy and science. After returning to France he became wealthy through speculation, and was thereafter able to devote himself to writing. He had several long and scandalous affairs, and travelled widely throughout Europe. In later life Voltaire campaigned vigorously for legal reform and against religious intolerance, in France and further afield.

Key works

1733 Philosophical Letters

1734 Treatise on Metaphysics

1759 Candide

1764 Philosophical Dictionary

## Empiricism

BEFORE

1637 René Descartes espouses rationalism in his Discourse on the Method.

1690 John Locke sets out the case for empiricism in An Essay Concerning Human Understanding.

AFTER

1781 Immanuel Kant is inspired by Hume to write his Critique of Pure Reason.

1844 Arthur Schopenhauer acknowledges his debt to Hume in The World as Will and Representation.

1934 Karl Popper proposes falsification as the basis for the scientific method, as opposed to observation and induction.

David Hume was born at a time when European philosophy was dominated by a debate about the nature of knowledge. René Descartes had in effect set the stage for modern philosophy in his Discourse on the Method, instigating a movement of rationalism in Europe, which claimed that knowledge can be arrived at by rational reflection alone. In Britain, John Locke had countered this with his empiricist argument that knowledge can only be derived from experience. George Berkeley had followed, formulating his own version of empiricism, according to which the world only exists in so far as it is perceived. But it was Hume, the third of the major British empiricists, who dealt the biggest blow to rationalism in an argument presented in his Treatise of Human Nature.

Hume’s fork

With a remarkable clarity of language, Hume turns a sceptical eye to the problem of knowledge, and argues forcibly against the notion that we are born with “innate ideas” (a central tenet of rationalism). He does so by first dividing the contents of our minds into two kinds of phenomena, and then asking how these relate to each other. The two phenomena are “impressions”—or direct perceptions, which Hume calls the “sensations, passions, and emotions”—and “ideas”, which are faint copies of our impressions, such as thoughts, reflections, and imaginings. And it is while analyzing this distinction that Hume draws an unsettling conclusion—one that calls into question our most cherished beliefs, not only about logic and science, but about the nature of the world around us.

The problem, for Hume, is that very often we have ideas that cannot be supported by our impressions, and Hume concerns himself with finding the extent to which this is the case. To understand what he means, we need to note that for Hume there are only two kinds of statement—namely “demonstrative” and “probable” statements—and he claims that in everyday experience we somehow confuse the two types of knowledge that these express.

A demonstrative statement is one whose truth or falsity is self-evident. Take, for example, the statement 2 + 2 = 4. Denying this statement involves a logical contradiction—in other words, to claim that 2 + 2 does not equal 4 is to fail to grasp the meanings of the terms “2” or “4” (or “+” or “=”). Demonstrative statements in logic, mathematics, and deductive reasoning are known to be true or false a priori, meaning “prior to experience.” The truth of a probable statement, however, is not self-evident, for it is concerned with matters of empirical fact. For example, any statement about the world such as “Jim is upstairs”, is a probable statement because it requires empirical evidence for it to be known to be true or false. In other words, its truth or falsity can only be known through some kind of experiment—such as by going upstairs to see if Jim is there.

In light of this, we can ask of any statement whether it is probable or demonstrative. If it is neither of these, then we cannot know it to be true or false, and so, for Hume, it is a meaningless statement. This division of all statements into two possible kinds, as if forming the horns of a dilemma, is often referred to as “Hume’s fork.”

"In our reasonings concerning fact, there are all imaginable degrees of assurance. A wise man therefore proportions his belief to the evidence."

David Hume

Mathematics and logic yield what Hume calls “demonstrative” truths, which cannot be denied without contradiction. These are the only certainties in Hume’s philosophy.

The grounds for our belief that the sun will rise tomorrow, or that water rather than fruit will flow from a faucet, are not logical, according to Hume. They are simply the result of our conditioning, which teaches us that tomorrow the world will be the same as it is today.

Inductive reasoning

There are no surprises in Hume’s reasoning so far, but things take a strange turn when he applies this line of argument to inductive inference—our ability to infer things from past evidence. We observe an unchanging pattern, and infer that it will continue in the future, tacitly assuming that nature will continue to behave in a uniform way. For example, we see the sun rise every morning, and infer that it will rise again tomorrow. But is our claim that nature follows this uniform pattern really justifiable? Claiming that the sun will rise tomorrow is not a demonstrative statement, as claiming the opposite involves no logical contradiction. Nor is it a probable statement, as we cannot experience the sun’s future risings.

The same problem occurs if we apply Hume’s fork to the evidence for causality. The statement “event A causes event B” seems on the face of it to be one that we can verify, but again, this does not stand up to scrutiny. There is no logical contradiction involved in denying that A causes B (as there would be in denying that 2 + 2 = 4), so it cannot be a demonstrative statement. Nor can it be proved empirically, since we cannot observe every event A to see if it is followed by B, so it is not a probable statement either. The fact that, in our limited experience, B invariably follows A is no rational ground for believing that A will always be followed by B, or that A causes B.

If there is never any rational basis for inferring cause and effect, then what justification do we have for making that connection? Hume explains this simply as “human nature”—a mental habit that reads uniformity into regular repetition, and a causal connection into what he calls the “constant conjunction” of events. Indeed, it is this kind of inductive reasoning that is the basis of science, and tempts us to interpret our inferences as “laws” of nature—but despite what we may think, this practice cannot be justified by rational argument.

In saying this, Hume makes his strongest case against rationalism, for he is saying that it is belief (which he defines as “a lively idea related to or associated with a present impression”), guided by custom, that lies at the heart of our claims to knowledge rather than reason.

"Nature, by an absolute and uncontrollable necessity, has determined us to judge as well as to breathe and feel."

David Hume

Custom as our guide

Hume goes on to acknowledge that although inductive inferences are not provable, this does not mean that they are not useful. After all, we still have a reasonable claim to expect something to happen, judging from past observation and experience. In the absence of a rational justification for inductive inference, custom is a good guide.

Hume adds, however, that this “mental habit” should be applied with caution. Before inferring cause and effect between two events, we should have evidence both that this succession of events has been invariable in the past, and that there is a necessary connection between them. We can reasonably predict that when we let go of an object it will fall to the ground, because this is what has always happened in the past, and there is an obvious connection between letting go of the object and its falling. On the other hand, two clocks set a few seconds apart will chime one after another—but since there is no obvious connection between them, we should not infer that one clock’s chiming is the cause of the other’s.

Hume’s treatment of the “problem of induction”, as this became known, both undermines the claims of rationalism and elevates the role of belief and custom in our lives. As he says, the conclusions drawn by our beliefs are “as satisfactory to the mind… as the demonstrative kind.”

Science supplies us with ever more detailed information about the world. However, according to Hume, science deals with theories only, and can never yield a “law of nature.”

A revolutionary idea

The brilliantly argued and innovative ideas in the Treatise of Human Nature were virtually ignored when they were published in 1739, despite being the high-point of British empiricism. Hume was better known in his own country for being the author of a History of Great Britain than for his philosophy; in Germany, however, the significance of his epistemology had more impact. Immanuel Kant admitted to being woken from his “dogmatic slumbers” by reading Hume, who remained a significant influence on German philosophers of the 19th century and the logical positivists of the 20th century, who believed that only meaningful statements could be verifiable. Hume’s account of the problem of induction remained unchallenged throughout this period, and resurfaced in the work of Karl Popper, who used it to back up his claim that a theory can only be deemed scientific if it is falsifiable.

"Hume was perfectly right in pointing out that induction cannot be logically justified."

Karl Popper

DAVID HUME

Born in Edinburgh, Scotland, in 1711, Hume was a precocious child who entered the University of Edinburgh at the age of 12. Around 1729 he devoted his time to finding “some medium by which truth might be established”, and after working himself into a nervous breakdown he moved to La Flèche in Anjou, France. Here he wrote A Treatise of Human Nature, setting out virtually all his philosophical ideas before returning to Edinburgh.

In 1763 he was appointed to the Embassy in Paris, where he befriended the philosopher Jean-Jacques Rousseau and became more widely known as a philosopher. The controversial Dialogues Concerning Natural Religion occupied Hume’s final years and, because of what he called his “abundant caution”, were only published after his death in Edinburgh in 1776.

Key works

1739 A Treatise of Human Nature

1748 An Enquiry Concerning Human Understanding

1779 Dialogues Concerning Natural Religion

## Idealism

BEFORE

1641 René Descartes discovers that it is impossible to doubt that “I exist.” The self is therefore the one and only thing of which we can be sure.

18th century Immanuel Kant develops a philosophy of idealism and the transcendental ego, the “I” that synthesizes information. This forms the basis of Fichte’s idealism and notion of the self.

AFTER

20th century Fichte’s nationalist ideas become associated with Martin Heidegger and the Nazi regime in Germany.

1950s Isaiah Berlin holds Fichte’s idea of true freedom of the self as responsible for modern authoritarianism.

Johann Gottlieb Fichte was an 18th-century German philosopher and student of Immanuel Kant. He examined how it is possible for us to exist as ethical beings with free will, while living in a world that appears to be causally determined; that is to say, in a world where every event follows on necessarily from previous events and conditions, according to unvarying laws of nature.

The idea that there is a world like this “out there”, beyond our selves and independent of us, is known as dogmatism. This is an idea that gained ground in the Enlightenment period, but Fichte thinks that it leaves no room for moral values or choice. How can people be considered to have free will, he asks, if everything is determined by something else that exists outside of ourselves?

Fichte argues instead for a version of idealism similar to Kant’s, in which our own minds create all that we think of as reality. In this idealist world, the self is an active entity or essence that exists outside of causal influences, and is able to think and choose freely, independently, and spontaneously.

Fichte understands idealism and dogmatism to be entirely different starting points. They can never be “mixed” into one philosophical system, he says; there is no way of proving philosophically which is correct, and neither can be used to refute the other. For this reason one can only “choose” which philosophy one believes in, not for objective, rational reasons, but depending upon “what sort of person one is.”

"Think the I, and observe what is involved in doing this."

Johann Gottlieb Fichte

## Pragmatism

BEFORE

17th century John Locke challenges rationalism by tracing the origin of our ideas to sense impressions.

18th century Immanuel Kant argues that speculation about what lies beyond our experience is meaningless.

AFTER

1890s William James and John Dewey take up the philosophy of pragmatism.

1920s Logical positivists in Vienna formulate the theory of verification—that the meaning of a statement is the method by which it is verified.

1980s Richard Rorty’s version of pragmatism argues that the very notion of truth can be dispensed with.

Charles Sanders Peirce was the scientist, logician, and philosopher of science who pioneered the philosophical movement known as pragmatism. Deeply sceptical of metaphysical ideas—such as the idea that there is a “real” world beyond the world we experience—he once asked his readers to consider what is wrong with the following theory: a diamond is actually soft, and only becomes hard when it is touched.

Peirce argued that there is “no falsity” in such thinking, for there is no way of disproving it. However, he claimed that the meaning of a concept (such as “diamond” or “hard”) is derived from the object or quality that the concept relates to—and the effects it has on our senses. Whether we think of the diamond as “soft until touched” or “always hard” before our experience, therefore, is irrelevant. Under both theories the diamond feels the same, and can be used in exactly the same way. However, the first theory is far more difficult to work with, and so is of less value to us.

This idea, that the meaning of a concept is the sensory effect of its object, is known as the pragmatic maxim, and it became the founding principle of pragmatism—the belief that the “truth” is the account of reality that works best for us.

One of the key things Peirce was trying to accomplish was to show that many debates in science, philosophy, and theology are meaningless. He claimed that they are often debates about words, rather than reality, because they are debates in which no effect on the senses can be specified.

"Nothing is vital for science; nothing can be."

Charles Sanders Peirce

## Pragmatism

BEFORE

1843 John Stuart Mill’s A System of Logic studies the ways in which we come to believe something is true.

1870s Charles Sanders Peirce describes his new pragmatist philosophy in How to Make Our Ideas Clear.

AFTER

1907 Henri Bergson’s Creative Evolution describes reality as a flow rather than a state.

1921 Bertrand Russell explores reality as pure experience in The Analysis of Mind.

1925 John Dewey develops a personal version of pragmatism, known as “instrumentalism”, in Experience and Nature.

Over the course of the 19th century, as the United States began to find its feet as an independent nation, philosophers from New England such as Henry David Thoreau and Ralph Waldo Emerson gave a recognizably American slant to European Romantic ideas. But it was the following generation of philosophers, who lived almost a century after the Declaration of Independence, that came up with something truly original.

The first of these, Charles Sanders Peirce, proposed a theory of knowledge he called pragmatism, but his work was hardly noticed at the time; it fell to his lifelong friend William James—godson to Ralph Emerson—to champion Peirce’s ideas and develop them further.

Truth and usefulness

Central to Peirce’s pragmatism was the theory that we do not acquire knowledge simply by observing, but by doing, and that we rely on that knowledge only so long as it is useful, in the sense that it adequately explains things for us. When it no longer fulfils that function, or better explanations make it redundant, we replace it. For example, we can see by looking back in history how our ideas about the world have changed constantly, from thinking that Earth is flat to knowing it to be round; from assuming that Earth is the center of the universe, to realizing that it is just one planet in a vast cosmos. The older assumptions worked perfectly adequately as explanations in their time, yet they are not true, and the universe itself has not changed. This demonstrates how knowledge as an explanatory tool is different from facts. Peirce examined the nature of knowledge in this way, but James was to apply this reasoning to the notion of truth.

For James, the truth of an idea depends on how useful it is; that is to say, whether or not it does what is required of it. If an idea does not contradict the known facts—such as laws of science—and it does provide a means of predicting things accurately enough for our purposes, he says there can be no reason not to consider it true, in the same way that Peirce considered knowledge as a useful tool irrespective of the facts. This interpretation of truth not only distinguishes it from fact, but also leads James to propose that “the truth of an idea is not a stagnant property inherent in it. Truth happens to an idea. It becomes true, is made true by events. Its verity is in fact an event, a process.” Any idea, if acted upon, is found to be true by the action we take; putting the idea into practice is the process by which it becomes true.

James also thinks that belief in an idea is an important factor in choosing to act upon it, and in this way belief is a part of the process that makes an idea true. If I am faced with a difficult decision, my belief in a particular idea will lead to a particular course of action and so contribute to its success. It is because of this that James defines “true beliefs” as those that prove useful to the believer. Again, he is careful to distinguish these from facts, which he says “are not true. They simply are. Truth is the function of the beliefs that start and terminate among them.”

"Every way of classifying a thing is but a way of handling it for some particular purpose."

William James

The idea of a flat Earth served well as a “truth” for several thousand years, despite the fact that Earth is a sphere. James claims that an idea’s usefulness determines its truthfulness.

The right to believe

Every time we try to establish a new belief, it would be useful if we had all the available evidence and the time to make a considered decision. But in much of life we do not have that luxury; either there is not enough time to examine the known facts, or there is not enough evidence, and we are forced to a decision. We have to rely on our beliefs to guide our actions, and James says that we have “the right to believe” in these cases.

James explains this by taking the example of a man lost and starving in a forest. When he sees a path, it is important for him to believe that the path will lead him out of the forest and to habitation, because if he does not believe it, he will not take the path, and will remain lost and starving. But if he does, he will save himself. By acting on his idea that the path will lead him to safety, it becomes true. In this way our actions and decisions make our belief in an idea become true. This is why James asserts “act as if what you do makes a difference”—to which he adds the typically concise and good-humored rider, “it does.”

We must, however, approach this idea with caution: a shallow interpretation of what James is saying could give the impression that any belief, no matter how outlandish, could become true by acting upon it—which of course is not what he meant. There are certain conditions that an idea must fulfil before it can be considered a justifiable belief. The available evidence must weigh in its favor, and the idea must be sufficient to withstand criticism. In the process of acting upon the belief, it must continually justify itself by its usefulness in increasing our understanding or predicting results. And even then, it is only in retrospect that we can safely say that the belief has become true through our acting upon it.

Religious belief can bring about extraordinary changes in people’s lives, such as the healing of the sick at places of pilgrimage. This occurs regardless of whether or not a god actually exists.

Reality as a process

James was a psychologist as well as a philosopher, and he sees the implications of his ideas in terms of human psychology as much as in the theory of knowledge. He recognized the psychological necessity for humans to hold certain beliefs, particularly religious ones. James thinks that while it is not justifiable as a fact, belief in a god is useful to its believer if it allows him or her to lead a more fulfilled life, or to overcome the fear of death. These things—a more fulfilled life and a fearless confrontation of death—become true; they happen as the result of a belief, and the decisions and actions based upon it.

Along with his pragmatic notion of truth, James proposes a type of metaphysics that he calls “radical empiricism.” This approach takes reality to be a dynamic, active process, in the same way that truth is a process. Like the traditional empiricists before him, James rejected the rationalist notion that the changing world is in some way unreal, but he also went further to state that “for pragmatism, [reality] is still in the making”, as truth is constantly being made to happen. This “stream” of reality, he believes, is not susceptible to empirical analysis either, both because it is in continual flux and because the act of observing it affects the truth of the analysis. In James’s radical empiricism, from which both mind and matter are formed, the ultimate stuff of reality is pure experience.

"The pragmatic method means looking away from principles and looking towards consequences."

William James

Continuing influence

Pragmatism, proposed by Peirce and expounded by James, established America as a significant center for philosophical thought in the 20th century. James’s pragmatic interpretation of truth influenced the philosophy of John Dewey, and spawned a “neopragmatist” school of thought in America that includes philosophers such as Richard Rorty. In Europe, Bertrand Russell and Ludwig Wittgenstein were indebted to James’s metaphysics. His work in psychology was equally influential, and often intimately connected with his philosophy, notably his concept of the “stream of consciousness”, which in turn influenced writers such as Virginia Woolf and James Joyce.

WILLIAM JAMES

Born in New York City, William James was brought up in a wealthy and intellectual family; his father was a famously eccentric theologian, and his brother Henry became a well-known author. During his childhood he lived for several years in Europe, where he pursued a love of painting, but at the age of 19 he abandoned this to study science. His studies at Harvard Medical School were interrupted by the ill health and depression that were to prevent him from ever practicing medicine, but he eventually graduated and in 1872 took a teaching post in physiology at Harvard University. His increasing interest in the subjects of psychology and philosophy led him to write acclaimed publications in these fields, and he was awarded a professorship in philosophy at Harvard in 1880. He taught there until his retirement in 1907.

Key works

1890 The Principles of Psychology

1896 The Will to Believe

1902 The Varieties of Religious Experience

1907 Pragmatism

## Vitalism

BEFORE

13th century John Duns Scotus distinguishes between intuitive and abstract thought, and claims that intuitive thought takes precedence.

1781 Immanuel Kant publishes Critique of Pure Reason, claiming that absolute knowledge is impossible.

AFTER

1890s William James begins to explore the philosophy of everyday experience, popularizing pragmatism.

1927 Alfred North Whitehead writes Process philosophy, suggesting that the existence of the natural world should be understood in terms of process and change, not things or fixed stabilities.

Henri Bergson’s 1910 book Creative Evolution explored his vitalism, or theory of life. In it, Bergson wanted to discover whether it is possible to really know something—not just to know about it, but to know it as it actually is.

Ever since the philosopher Immanuel Kant published The Critique of Pure Reason in 1781, many philosophers have claimed that it is impossible for us to know things as they actually are. This is because Kant showed that we can know how things are relative to we ourselves, given the kinds of minds we have; but we can never step outside of ourselves to achieve an absolute view of the world’s actual “things-in-themselves.”

Two forms of knowledge

Bergson, however, does not agree with Kant. He says that there are two different kinds of knowledge: relative knowledge, which involves knowing something from our own unique particular perspective; and absolute knowledge, which is knowing things as they actually are. Bergson believes that these are reached by different methods, the first through analysis or intellect, and the second through intuition. Kant’s mistake, Bergson believes, is that he does not recognize the full importance of our faculty of intuition, which allows us to grasp an object’s uniqueness through direct connection. Our intuition is linked to what Bergson called our élan vital, a life-force (vitalism) that interprets the flux of experience in terms of time rather than space.

Suppose you want to get to know a city, he says. You could compile a record of it by taking photographs of every part, from every possible perspective, before reconstructing these images to give some idea of the city as a whole. But you would be grasping it at one remove, not as a living city. If, on the other hand, you were simply to stroll around the streets, paying attention in the right way, you might acquire knowledge of the city itself—a direct knowledge of the city as it actually is. This direct knowledge, for Bergson, is knowledge of the essence of the city.

But how do we practice intuition? Essentially, it is a matter of seeing the world in terms of our sense of unfolding time. While walking through the city, we have a sense of our own inner time, and we also have an inner sense of the various unfolding times of the city through which we are walking. As these times overlap, Bergson believes that we can make a direct connection with the essence of life itself.

Capturing the essence of a city, person, or object may only be possible through direct knowledge gained from intuition, not analysis. Bergson says we underestimate the value of our intuition.

HENRI BERGSON

Henri Bergson was one of the most influential French philosophers of his time. Born in France in 1859, he was the son of an English mother and a Polish father. His early intellectual interests lay in mathematics, at which he excelled. Despite this, he took up philosophy as a career, initially teaching in schools. When his book Matter and Memory was published in 1896, he was elected to the Collège de France and became a university lecturer. He also had a successful political career, and represented the French government during the establishment of the League of Nations in 1920. His work was widely translated and influenced many other philosophers and psychologists, including William James. He was awarded the Nobel Prize for Literature in 1927, and died in 1941 at the age of 81.

Key works

1896 Matter and Memory

1903 An Introduction to Metaphysics

1907 Creative Evolution

1932 The Two Sources of Morality and Religion

## Pragmatism

BEFORE

1859 Charles Darwin’s On the Origin of Species puts human beings in a new, naturalistic perspective.

1878 Charles Sanders Peirce’s essay How to Make our Ideas Clear lays the foundations of the pragmatist movement.

1907 William James publishes Pragmatism: A New Name for Some Old Ways of Thinking, popularizing the philosophical term “pragmatism.”

AFTER

From 1970 Jürgen Habermas applies pragmatic principles to social theory.

1979 Richard Rorty combines pragmatism with analytic philosophy in Philosophy and the Mirror of Nature.

John Dewey belongs to the philosophical school known as pragmatism, which arose in the US in the late 19th century. The founder is generally considered to be the philosopher Charles Sanders Peirce, who wrote a groundbreaking essay in 1878 called How to Make our Ideas Clear.

Pragmatism starts from the position that the purpose of philosophy, or “thinking”, is not to provide us with a true picture of the world, but to help us to act more effectively within it. If we are taking a pragmatic perspective, we should not be asking “is this the way things are?” but rather, “what are the practical implications of adopting this perspective?”

For Dewey, philosophical problems are not abstract problems divorced from people’s lives. He sees them as problems that occur because humans are living beings trying to make sense of their world, struggling to decide how best to act within it. Philosophy starts from our everyday human hopes and aspirations, and from the problems that arise in the course of our lives. This being the case, Dewey thinks that philosophy should also be a way of finding practical responses to these problems. He believes that philosophizing is not about being a “spectator” who looks at the world from afar, but about actively engaging in the problems of life.

Evolving creatures

Dewey was strongly influenced by the evolutionary thought of the naturalist Charles Darwin, who published On The Origin of Species in 1859. Darwin described humans as living creatures who are a part of the natural world. Like the other animals, humans have evolved in response to their changing environments. For Dewey, one of the implications of Darwin’s thought is that it requires us to think of human beings not as fixed essences created by God, but instead as natural beings. We are not souls who belong in some other, non-material world, but evolved organisms who are trying to do our best to survive in a world of which we are inescapably a part.

Everything changes

Dewey also takes from Darwin the idea that nature as a whole is a system that is in a constant state of change; an idea that itself echoes the philosophy of the ancient Greek philosopher Heraclitus. When Dewey comes to think about what philosophical problems are, and how they arise, he takes this insight as a starting point.

Dewey discusses the idea that we only think when confronted with problems in an essay entitled Kant and the Philosophic Method (1884). We are, he says, organisms that find ourselves having to respond to a world that is subject to constant change and flux. Existence is a risk, or a gamble, and the world is fundamentally unstable. We depend upon our environment to be able to survive and thrive, but the many environments in which we find ourselves are themselves always changing. Not only this, but these environments do not change in a predictable fashion. For several years there may be a good crop of wheat, for instance, but then the harvest fails. A sailor may set sail under fine weather, only to find that a storm suddenly blows up out of nowhere. We are healthy for years, and then disease strikes us when we least expect it.

In the face of this uncertainty, Dewey says that there are two different strategies we can adopt. We can either appeal to higher beings and hidden forces in the universe for help, or we can seek to understand the world and gain control of our environment.

"We do not solve philosophical problems, we get over them."

John Dewey

Appeasing the gods

The first of these strategies involves attempting to affect the world by means of magical rites, ceremonies, and sacrifices. This approach to the uncertainty of the world, Dewey believes, forms the basis of both religion and ethics.

In the story that Dewey tells, our ancestors worshipped gods and spirits as a way of trying to ally themselves with the “powers that dispense fortune.” This scenario is played out in stories from around the world, in myths and legends such as those about unfortunate seafarers who pray to gods or saints to calm the storm, and thereby survive. In the same way, Dewey believes, ethics arises out of the attempts our ancestors made to appease hidden forces; but where they made sacrifices, we strike bargains with the gods, promising to be good if they spare us from harm.

The alternative response to the uncertainties of our changing world is to develop various techniques of mastering the world, so that we can live in it more easily. We can learn the art of forecasting the weather, and build houses to shelter ourselves from its extremes, and so on. Rather than attempting to ally ourselves with the hidden powers of the universe, this strategy involves finding ways of revealing how our environment works, and then working out how to transform it to our benefit.

Dewey points out that it is important to realize that we can never completely control our environment or transform it to such an extent that we can drive out all uncertainty. At best, he says, we can modify the risky, uncertain nature of the world in which we find ourselves. But life is inescapably risky.

We no longer employ sacrifice as a way to ask for help from the gods, but many people find themselves offering up a silent promise to be good in return for help from some higher being.

A luminous philosophy

For much of human history, Dewey writes, these two approaches to dealing with the riskiness of life have existed in tension with each other, and they have given rise to two different kinds of knowledge: on the one hand, ethics and religion; and on the other hand, arts and technologies. Or, more simply, tradition and science. Philosophy, in Dewey’s view, is the process by means of which we try to work through the contradictions between these two different kinds of response to the problems in our lives. These contradictions are not just theoretical; they are also practical. For example, I may have inherited innumerable traditional beliefs about ethics, meaning, and what constitutes a “good life”, but I may find that these beliefs are in tension with the knowledge and understanding that I have gained from studying the sciences. In this context philosophy can be seen as the art of finding both theoretical and practical responses to these problems and contradictions.

There are two ways in which to judge whether a form of philosophy is successful. First, we should ask whether it has made the world more intelligible. Does this particular philosophical theory make our experience “more luminous”, Dewey asks, or does it make it “more opaque”? Here Dewey is agreeing with Peirce that philosophy’s purpose is to make our ideas and our everyday experience clearer and easier to understand. He is critical of any philosophical approaches that ultimately make our experience more puzzling, or the world more mysterious.

Second, he thinks we should judge a philosophical theory by asking to what extent it succeeds in addressing the problems of living. Is it useful to us, in our everyday lives? Does it, for instance, “yield the enrichment and increase of power” that we have come to expect from new scientific theories?

Scientific experiments, such as those performed by Benjamin Franklin in the 1740s, help us gain control over the world. Dewey thought philosophical theories should be equally useful.

A practical influence

A number of philosophers, such as Bertrand Russell, have criticized pragmatism by claiming that it has simply given up on the long philosophical quest for truth. Nevertheless, Dewey’s philosophy has been enormously influential in America. Given that Dewey places such an overriding emphasis on responding to the practical problems of life, it is perhaps unsurprising that much of his influence has been in practical realms, such as in education and in politics.

"Education is not an affair of telling and being told, but an active and constructive process."

John Dewey

JOHN DEWEY

John Dewey was born in Vermont, USA, in 1859. He studied at the University of Vermont, and then worked as a schoolteacher for three years before returning to undertake further study in psychology and philosophy. He taught at various leading universities for the remainder of his life, and wrote extensively on a broad range of topics, from education to democracy, psychology, and art. In addition to his work as a scholar, he set up an educational institution—the University of Chicago Laboratory Schools—which put into practice his educational philosophy of learning by doing. This institution is still running today. Dewey’s broad range of interests, and his abilities as a communicator, allowed his influence on American public life to extend far beyond the Laboratory Schools. He wrote about philosophy and social issues until he died in 1952 at the age of 92.

Key works

1910 How We Think

1925 Experience and Nature

1929 The Quest for Certainty

1934 Art as Experience

## Existentialism

BEFORE

1800s Søren Kierkegaard writes of philosophy as a matter of the individual’s struggle with truth.

1880s Friedrich Nietzsche says that “God is dead”, there are no absolute truths, and we must rethink all our values.

1920s Martin Heidegger claims that philosophy is a matter of our relationship with our own existence.

AFTER

From 1940 Hannah Arendt’s ideas of freedom are influenced by Jaspers’ philosophy.

From 1950 Hans-Georg Gadamer explores the idea that philosophy progresses through a fusion of individual perspectives.

For some, philosophy is a way to discover objective truths about the world. For German philosopher and psychiatrist Karl Jaspers, on the other hand, philosophy is a personal struggle. Strongly influenced by the philosophers Kierkegaard and Nietzsche, Jaspers is an existentialist who suggests that philosophy is a matter of our own attempts to realize truth. Since philosophy is an individual struggle, he writes in his 1941 book On my Philosophy, we can philosophize only as individuals. We cannot turn to anybody else to tell us the truth; we must discover it for ourselves, by our own efforts.

The philosopher lives in the invisible realm of the spirit, struggling to realize truth. The thoughts of other, companion, philosophers act as signposts towards potential paths to understanding.

A community of individuals

Although in this sense truth is something that we realize alone, it is in communication with others that we realize the fruits of our efforts and raise our consciousness beyond its limits. Jaspers considers his own philosophy “true” only so far as it aids communication with others. And while other people cannot provide us with a form of “ready-made truth”, philosophy remains a collective endeavor. For Jaspers, each individual’s search

for truth is carried out in community with all those “companions in thought” who have undergone the same personal struggle.

## Phenomenology

BEFORE

4th century BCE Aristotle claims that philosophy begins with a sense of wonder.

1641 René Descartes’ Meditations on First Philosophy establishes a form of mind–body dualism that Merleau-Ponty will reject.

Early 1900s Edmund Husserl founds phenomenology as a philosophical school.

1927 Martin Heidegger writes Being and Time, a major influence on Merleau-Ponty.

AFTER

1979 Hubert Dreyfus draws on the works of Heidegger, Wittgenstein, and Merleau-Ponty to explore philosophical problems raised by artificial intelligence and robotics.

The idea that philosophy begins with our ability to wonder at the world goes back as far as ancient Greece. Usually we take our everyday lives for granted, but Aristotle claimed that if we want to understand the world more deeply, we have to put aside our familiar acceptance of things. And nowhere, perhaps, is this harder to do than in the realm of our experience. After all, what could be more reliable than the facts of direct perception?

French philosopher Merleau-Ponty was interested in looking more closely at our experience of the world, and in questioning our everyday assumptions. This puts him in the tradition known as phenomenology, an approach to philosophy pioneered by Edmund Husserl at the beginning of the 20th century. Husserl wanted to explore first-person experience in a systematic way, while putting all assumptions about it to one side.

"Man is in the world and only in the world does he know himself."

Maurice Merleau-Ponty

The body-subject

Merleau-Ponty takes up Husserl’s approach, but with one important difference. He is concerned that Husserl ignores what is most important about our experience—the fact that it consists not just of mental experience, but also of bodily experience. In his most important book, The Phenomenology of Perception, Merleau-Ponty explores this idea and comes to the conclusion that the mind and body are not separate entities—a thought that contradicts a long philosophical tradition championed by Descartes. For Merleau-Ponty, we have to see that thought and perception are embodied, and that the world, consciousness, and the body are all part of a single system. And his alternative to the disembodied mind proposed by Descartes is what he calls the body-subject. In other words, Merleau-Ponty rejects the dualist’s view that the world is made of two separate entities, called mind and matter.

Cognitive science

Because he was interested in seeing the world anew, Merleau-Ponty took an interest in cases of abnormal experience. For example, he believed that the phantom limb phenomenon (in which an amuptee “feels” his missing limb) shows that the body cannot simply be a machine. If it were, the body would no longer acknowledge the missing part—but it still exists for the subject because the limb has always been bound up with the subject’s will. In other words, the body is never “just” a body—it is always a “lived” body.

Merleau-Ponty’s focus on the role of the body in experience, and his insights into the nature of the mind as fundamentally embodied, have led to a revival of interest in his work among cognitive scientists. Many recent developments in cognitive science seem to bear out his idea that, once we break with our familiar acceptance of the world, experience is very strange indeed.

MRI scans of the brain provide doctors with life-saving information. However, in Merleau-Ponty’s view, no amount of physical information can give us a complete account of experience.

MAURICE MERLEAU-PONTY

Maurice Merleau-Ponty was born in Rochefort-sur-Mer, France, in 1908. He attended the École Normale Supérieure along with Jean-Paul Sartre and Simone de Beauvoir, and graduated in philosophy in 1930. He worked as a teacher at various schools, until joining the infantry during World War II. His major work, The Phenomenology of Perception, was published in 1945, after which he taught philosophy at the University of Lyon.

Merleau-Ponty’s interests extended beyond philosophy to include subjects such as education and child psychology. He was also a regular contributor to the journal Les Temps modernes. In 1952, Merleau-Ponty became the youngest-ever Chair of Philosophy at the College de France, and remained in the post until his death in 1961, at the age of only 53.

Key works

1942 The Structure of Behaviour

1945 The Phenomenology of Perception

1964 The Visible and the Invisible

## Existentialism

BEFORE

1843 Søren Kierkegaard explores the idea of the absurd in his book, Fear and Trembling.

1864 Russian writer Fyodor Dostoyevsky publishes Notes from the Underground, which has existentialist themes.

1901 Friedrich Nietzsche writes in Will to Power that “our existence (action, suffering, willing, feeling) has no meaning.”

1927 Martin Heidegger’s Being and Time lays the ground for the development of existential philosophy.

AFTER

1971 Philosopher Thomas Nagel argues that absurdity arises out of a contradiction within us.

Some people believe that philosophy’s task is to search for the meaning of life. But the French philosopher and novelist Albert Camus thought that philosophy should recognize instead that life is inherently meaningless. While at first this seems a depressing view, Camus believes that only by embracing this idea are we capable of living as fully as possible.

Camus’ idea appears in his essay The Myth of Sisyphus. Sisyphus was a Greek king who fell out of favor with the gods, and so was sentenced to a terrible fate in the Underworld. His task was to roll an enormous rock to the top of a hill, only to watch it roll back to the bottom. Sisyphus then had to trudge down the hill to begin the task again, repeating this for all eternity. Camus was fascinated by this myth, because it seemed to him to encapsulate something of the meaninglessness and absurdity of our lives. He sees life as an endless struggle to perform tasks that are essentially meaningless.

Camus recognizes that much of what we do certainly seems meaningful, but what he is suggesting is quite subtle. On the one hand, we are conscious beings who cannot help living our lives as if they are meaningful. On the other hand, these meanings do not reside out there in the universe; they reside only in our minds. The universe as a whole has no meaning and no purpose; it just is. But because, unlike other living things, we have consciousness, we are the kinds of beings who find meaning and purpose everywhere.

Recognizing the absurd

The absurd, for Camus, is the feeling that we have when we recognize that the meanings we give to life do not exist beyond our own consciousness. It is the result of a contradiction between our own sense of life’s meaning, and our knowledge that nevertheless the universe as a whole is meaningless.

Camus explores what it might mean to live in the light of this contradiction. He claims that it is only once we can accept the fact that life is meaningless and absurd that we are in a position to live fully. In embracing the absurd, our lives become a constant revolt against the meaninglessness of the universe, and we can live freely.

This idea was further developed by the philosopher Thomas Nagel, who said that the absurdity of life lies in the nature of consciousness, because however seriously we take life, we always know that there is some perspective from which this seriousness can be questioned.

Sisyphus was condemned eternally to push a rock up a hill, but Camus thought he might find freedom even in this grim situation if he accepted the meaninglessness of his eternal task.

"The struggle towards the heights is enough to fill a man’s heart."

Albert Camus

ALBERT CAMUS

Camus was born in Algeria in 1913. His father was killed a year later in World War I, and Camus was brought up by his mother in extreme poverty. He studied philosophy at the University of Algiers, where he suffered the first attack of the tuberculosis which was to recur throughout his life. At the age of 25 he went to live in France, where he became involved in politics. He joined the French Communist Party in 1935 but was expelled in 1937. During World War II he worked for the French Resistance, editing an underground newspaper and writing many of his best-known novels, including The Stranger. He wrote many plays, novels, and essays, and was awarded the Nobel Prize for Literature in 1957. Camus died in a car crash aged 46, having discarded a train ticket to accept a lift back to Paris with a friend.

Key works

1942 The Myth of Sisyphus

1942 The Stranger

1947 The Plague

1951 The Rebel

1956 The Fall

## Postmodernism

BEFORE

1870s The term “postmodern” is first used in the context of art criticism.

1939–45 Technological advances in World War II lay the ground for the computer revolution of the 20th century.

1953 Ludwig Wittgenstein writes in his Philosophical Investigations about “language games”—an idea that Lyotard uses to develop his idea of meta-narratives.

AFTER

1984 American literary critic Fredric Jameson writes Postmodernism, or the Cultural Logic of Late Capitalism.

From 1990s The World Wide Web offers unprecedented access to information.

The idea that knowledge is produced to be sold appears in Jean-François Lyotard’s book The Postmodern Condition: A Report on Knowledge. The book was originally written for the Council of Universities in Quebec, Canada, and the use of the term “postmodern” in its title is significant. Although Lyotard did not invent the term, which had been used by various art critics since the 1870s, his book was responsible for broadening its range and increasing its popularity. His use of the word in the title of this book is often said to mark the beginning of postmodern thought.

The term “postmodernism” has since been used in so many different ways that it is now hard to know exactly what it means, but Lyotard’s definition is very clear. Postmodernism, he writes, is a matter of “incredulity towards meta-narratives.” Meta-narratives are overarching, single stories that attempt to sum up the whole of human history, or that attempt to put all of our knowledge into a single framework. Marxism (the view that history can be seen as a series of struggles between social classes) is an example of a meta-narrative. Another is the idea that humanity’s story is one of progress toward deeper knowledge and social justice, brought about by greater scientific understanding.

Externalized knowledge

Our incredulity toward these meta- narratives implies a new scepticism. Lyotard suggests that this is due to a shift in the way we have related to knowledge since World War II, and to the huge change in the technologies we use to deal with it. Computers have fundamentally transformed our attitudes, as knowledge has become information that can be stored in databases, moved to and fro, and bought and sold. This is what Lyotard calls the “mercantilization” of knowledge.

This has several implications. The first, Lyotard points out, is that knowledge is becoming externalized. It is no longer something that helps toward the development of minds; something that might be able to transform us. Knowledge is also becoming disconnected from questions of truth. It is being judged not in terms of how true it is, but in terms of how well it serves certain ends. When we cease to ask questions about knowledge such as “is it true?” and start asking questions such as “how can this be sold?”, knowledge becomes a commodity. Lyotard is concerned that once this happens, private corporations may begin to seek to control the flow of knowledge, and decide who can access what types of knowledge, and when.

When knowledge becomes data it is no longer the indefinable matter of minds, but a commodity that can be transferred, stored, bought, or sold.

JEAN-FRANÇOIS LYOTARD

Jean-François Lyotard was born in Versailles, France in 1924. He studied philosophy and literature at the Sorbonne, Paris, becoming friends with Gilles Deleuze. After graduating, he taught philosophy in schools for several years in France and Algeria.

Lyotard became involved in radical left-wing politics in the 1950s, and was a well-known defender of the 1954–62 Algerian revolution, but his philosophical development ultimately led him to become disillusioned with the meta-narratives of Marxism. In the 1970s he began working as a university professor, teaching philosophy first at the Sorbonne and then in many other countries around the world, including the US, Canada, Brazil, and France. Lyotard retired as Professor Emeritus at the University of Paris VIII, and died of leukemia in 1998.

Key works

1971 Discourse, Figure

1974 Libidinal Economy

1979 The Postmodern Condition: A Report on Knowledge

1983 The Differend

## Discursive archaeology

BEFORE

Late 18th century Immanuel Kant lays the foundation for the 19th-century model of “man.”

1859 Charles Darwin’s On the Origin of Species causes a revolution in how we understand ourselves.

1883 Friedrich Nietzsche, in Thus Spoke Zarathustra, announces that man is something to be surpassed.

AFTER

1985 American philosopher Donna Haraway’s A Cyborg Manifesto attempts to imagine a post-human future.

1991 Daniel Dennett’s Consciousness Explained calls into question many of our most cherished notions about consciousness.

The idea that man is an invention of recent date appears in The Order of Things: An Archaeology of the Human Sciences by French philosopher Michel Foucault. To understand what Foucault means by this, we need to know what he means by archaeology, and why he thinks that we should apply it to the history of thought.

Foucault is interested in how our discourse—the way in which we talk and think about things—is formed by a set of largely unconscious rules that arise out of the historical conditions in which we find ourselves. What we take to be the “common sense” background to how we think and talk about the world is in fact shaped by these rules and these conditions. However, the rules and conditions change over time, and consequently so do our discourses. For this reason, an “archaeology” is needed to unearth both the limits and the conditions of how people thought and talked about the world in previous ages. We cannot take concepts that we use in our present context (for example, the concept of “human nature”) and assume that they are somehow eternal, and that all we need is a “history of ideas” to trace their genealogy. For Foucault, it is simply wrong to assume that our current ideas can be usefully applied to any previous point in history. The ways in which we use the words “man”, “mankind”, and “human nature”, Foucault believes, are examples of this.

The roots of this idea lie firmly in the philosophy of Immanuel Kant, who turned philosophy on its head by abandoning the old question “Why is the world the way it is?” and asking “Why do we see the world the way we do?” We take our idea of what it is to be human as fundamental and unchanging, but it is in fact only a recent invention. Foucault locates the beginning of our particular idea of “man” at the beginning of the 19th century, around the time of the birth of the natural sciences. This idea of “man” is, Foucault considers, paradoxical: we see ourselves both as objects in the world, and so as objects of study, and as subjects who experience and study the world—strange creatures that look in two directions at once.

The human self-image

Foucault suggests that not only is this idea of “man” an invention of recent date, it is also an invention that may be close to coming to its end—one that may soon be erased “like a face drawn in the sand at the edge of the sea.”

Is Foucault right? In a time of rapid advances in computing and human-machine interfaces, and when philosophers informed by cognitive science, such as Daniel Dennett and Dan Wegner, are questioning the very nature of subjectivity, it is hard not to feel that, even if the face in the sand is not about to be erased, the tide is lapping alarmingly at its edges.

"Man is neither the oldest nor the most constant problem that has been posed for human knowledge."

Michel Foucault

The 19th century saw a revolution in anatomy, as shown in this illustration from a medical text book. Foucault believes that our modern concept of man dates from this period.

MICHEL FOUCAULT

Foucault was born in Poitiers, France, in 1926 to a family of doctors. After World War II, he entered the École Normale Supérieure, where he studied philosophy under Maurice Merleau-Ponty. In 1954 he spent time in Uppsala, Sweden, and then lived for a time both in Poland and Germany, only returning to France in 1960.

He received a PhD in 1961 for his study A History of Madness, which argued that the distinction between madness and sanity is not real, but a social construct. After the month-long student strikes in Paris of 1968, he became involved in political activism, and continued to work both as a lecturer and an activist for the rest of his life.

Key works

1961 A History of Madness

1963 The Birth of the Clinic: An Archaeology of Medical Perception

1966 The Order of Things: An Archaeology of the Human Sciences

1975 Discipline and Punish: The Birth of the Prison

## Deconstruction

BEFORE

4th century BCE Plato’s Meno explores the idea of “aporia.”

Early 20th century Charles Sanders Peirce and Ferdinand de Saussure begin the study of signs and symbols (semiotics), which would become a key influence on Of Grammatology.

1961 Emmanuel Levinas publishes Totality and Infinity, which Derrida would respond to in Writing and Difference. Levinas becomes a growing influence in Derrida’s later explorations of ethics.

AFTER

1992 English philosopher Simon Critchley’s Ethics of Deconstruction explores aspects of Derrida’s work.

Jacques Derrida remains one of the most controversial 20th-century philosophers. His name is associated, first and foremost, with “deconstruction”, a complex and nuanced approach to how we read and understand the nature of written texts. If we are to understand what Derrida means when he says in his famous book Of Grammatology that there is nothing outside of the text (the original French is “il n’y a pas de hors-texte”, also translated as “there is no outside-text”), we need to take a closer look at Derrida’s deconstructive approach in general. Often when we pick up a book, whether a philosophy book or a novel, we imagine that what we have in our hands is something that we can understand or interpret as a relatively self-contained whole. When it comes to philosophical texts, we might be expected to imagine that these are especially systematic and logical. Imagine that you go into a bookshop and pick up a copy of Of Grammatology. You would think that, if you were to read the book, by the end of it you would have a reasonable grasp of what “grammatology” itself might be, what Derrida’s main ideas were on the subject, and what this said about the world. But, for Derrida, texts do not work in this way.

"We are all mediators, translators."

Jacques Derrida

Aporia and différance

Even the most straightforward texts (and Of Grammatology is not one such text) are riddled with what Derrida calls “aporias.” The word “aporia” comes from the Ancient Greek, where it means something like “contradiction”, “puzzle”, or “impasse.” For Derrida, all written texts have such gaps, holes, and contradictions and his method of deconstruction is a way of reading texts while looking out for these puzzles and impasses. In exploring these contradictions as they appear in different texts, Derrida aims to broaden our understanding of what texts are and what they do, and to show the complexity that lies behind even the most apparently simple works. Deconstruction is a way of reading texts to bring these hidden paradoxes and contradictions out into the open. This is not, however, just a matter of how we read philosophy and literature; there are much broader implications to Derrida’s approach that bring into question the relationship between language, thought, and even ethics.

At this point, it would help to introduce an important technical term from Derrida’s vocabulary: “différance.” This may look like a typographical error—and indeed, when the term différance first entered the French dictionary, the story goes that even Derrida’s mother sternly said to him, “But Jacques, that is not how you spell it!” But in fact différance is a word that Derrida coined himself to point to a curious aspect of language.

“Différance” (with an “a”) is a play both on the French “différence” (with an “e”), meaning “to differ”, and the French “deférrer” meaning “to defer.” To understand how this word works, it would be useful to consider how this deferring and differing might actually take place in practice. Let us start with deferring. Imagine that I say “The cat…”, then I add, “that my friend saw….” After a pause, I say, “in the garden was black and white…”, and so on. The precise meaning of the word “cat” as I am using it is continually deferred, or put off, as more information is given. If I had been cut off after saying “The cat…” and had not mentioned my friend or the garden, the meaning of “cat” would have been different. The more I add to what I say, in other words, the more the meaning of what I have already said is revised. Meaning is deferred in language.

But there is something else going on as well. The meaning of “cat”, Derrida believes, cannot be considered as something that rests in the relationship between my words and actual things in the world. The word takes its meaning from its position in a whole system of language. So when I say “cat”, this is meaningful not because of some mysterious link between the word and an actual cat, but because this term differs from, for example, “dog” or “lion” or “zebra.”

Taken together, these two ideas of deferring and differing say something quite strange about language in general. On the one hand, the meaning of anything we say is ultimately always deferred, because it depends on what else we say; and the meaning of that, in turn, depends on what else we say, and so on. And on the other hand, the meaning of any particular term we use depends on all the things that we don’t mean. So meaning is not self-contained within the text itself.

A typesetter can check plates of type closely before they are printed, but the ideas they express are full of “aporias”, or contradictions, says Derrida, which no amount of analysis can eliminate.

The meaning of what we write is, for Derrida, changed by what we write next. Even the deceptively simple act of writing a letter can lead to a deferral of meaning in the text itself.

The written word

For Derrida, différance is an aspect of language that we become aware of thanks to writing. Since ancient Greek times, philosophers have been suspicious of written language. In Plato’s dialogue, the Phaedrus, Socrates tells a legend about the invention of writing, and says that writing provides only “the appearance of wisdom” and not its reality. Writing, when philosophers have thought about it at all, has tended to be seen simply as a pale reflection of the spoken word; the latter has been taken as the primary means of communication. Derrida wants to reverse this; according to him, the written word shows us something about language that the spoken word does not.

The traditional emphasis on speech as a means of transmitting philosophical ideas has fooled us all, Derrida believes, into thinking that we have immediate access to meaning. We think that meaning is about “presence”—when we speak with another person, we imagine that they make their thoughts “present” for us, and that we are doing the same for them. If there is any confusion, we ask the other person to clarify. And if there are any puzzles, or aporias, we either ask for clarification, or these simply slide past us without our noticing. This leads us to think that meaning in general is about presence—to think, for example, that the real meaning of “cat” can be found in the presence of a cat on my lap.

But when we deal with a written text, we are freed from this naïve belief in presence. Without the author there to make their excuses and explain for us, we start to notice the complexities and the puzzles and the impasses. All of a sudden, language begins to look a little more complicated.

"We think only in signs."

Jacques Derrida

Questioning meaning

When Derrida says that there is nothing outside of the text, he does not mean that all that matters is the world of books, that somehow the world “of flesh and bone” does not matter. Nor is he trying to play down the importance of any social concerns that might lie behind the text. So what exactly is he saying?

First, Derrida is suggesting that if we take seriously the idea that meaning is a matter of différance, of differing and of deferring, then if we want to engage with the question of how we ought think about the world, we must always keep alive to the fact that meaning is never as straightforward as we think it is, and that this meaning is always open to being examined by deconstruction.

Second, Derrida is suggesting that in our thinking, our writing, and our speaking, we are always implicated in all manner of political, historical, and ethical questions that we may not even recognize or acknowledge. For this reason, some philosophers have suggested that deconstruction is essentially an ethical practice. In reading a text deconstructively, we call into question the claims that it is making, and we open up difficult ethical issues that may have remained hidden. Certainly in his later life, Derrida turned his attention to some of the very real ethical puzzles and contradictions that are raised by ideas such as “hospitality” and “forgiveness.”

Derrida’s own thesis that there is nothing outside of the text is open to be analyzed using his own deconstructive methods. Even the idea as explained in this book is subject to différance.

Derrida registered his opposition to the Vietnam War in a lecture given in the US in 1968. His involvement in numerous political issues and debates informed much of his later work.

Critics of Derrida

Given that Derrida’s idea is based on the notion that meaning can never be completely present in the text, it is perhaps not surprising that Derrida’s work can often be difficult. Michel Foucault, one of Derrida’s contemporaries, attacked Derrida’s thinking for being wilfully obscure; he protested that often it was impossible to say exactly what Derrida’s thesis actually was. The latter’s response to this, perhaps, might be to say that the idea of having a thesis is itself based on the idea of “presence” that he is attempting to call into question. This may seem like dodging the issue; but if we take Derrida’s idea seriously, then we have to admit that the idea that there is nothing outside of the text is itself not outside of the text. To take this idea seriously, then, is to treat it sceptically, to deconstruct it, and to explore the puzzles, impasses, and contradictions that—according to Derrida himself—lurk within it.

"I never give in to the temptation to be difficult just for the sake of being difficult."

Jacques Derrida

JACQUES DERRIDA

Jacques Derrida was born to Jewish parents in the then French colony of Algeria. He was interested in philosophy from an early age, but also nurtured dreams of becoming a professional soccer player. Eventually it was philosophy that won out and, in 1951, he entered the École Normale Supérieure in Paris. There he formed a friendship with Louis Althusser, also of Algerian origin, who, like Derrida, went on to become one of the most prominent thinkers of his day.

The publication in 1967 of Of Grammatology, Writing and Difference, and Speech and Phenomena sealed Derrida’s international reputation. A regular visiting lecturer at a number of European and American universities, he took up the post of Professor of Humanities at the University of California, Irvine, in 1986. His later work increasingly focused on issues of ethics, partly due to the influence of Emmanuel Levinas.

Key works

1967 Of Grammatology

1967 Writing and Difference

1967 Speech and Phenomena

1994 The Politics of Friendship

## Feminism

BEFORE

1949 Simone de Beauvoir’s The Second Sex explores the philosophical implications of sexual difference.

1962 French anthropologist Claude Lévi-Strauss writes The Savage Mind, a study of binary oppositions in culture.

1967 Controversial French philosopher Jacques Derrida publishes Of Grammatology, introducing the concept of deconstruction, which Cixous uses in her study of gender.

AFTER

1970s The French literary movement of écriture féminine (“women’s writing”) explores appropriate use of language in feminist thinking, taking its inspiration from Cixous.

In 1975, the French poet, novelist, playwright, and philosopher Hélène Cixous wrote Sorties, her influential exploration of the oppositions that often define the way we think about the world. For Cixous, a thread that runs through centuries of thought is our tendency to group elements of our world into opposing pairs, such as culture/nature, day/night, and head/heart. Cixous claims that these pairs of elements are always by implication ranked hierarchically, underpinned by a tendency to see one element as being dominant or superior and associated with maleness and activity, while the other element or weaker aspect is associated with femaleness and passivity.

Time for change

Cixous believes that the authority of this hierarchical pattern of thinking is now being called into question by a new blossoming of feminist thought. She questions what the implications of this change might be, not only for our philosophical systems, but also for our social and political institutions. Cixous herself, however, refuses to play the game of setting up binary oppositions, of victors and losers, as a structural framework for our thinking. Instead she conjures up the image of “millions of species of mole as yet not recognized”, tunnelling away under the edifices of our world view. And what will happen when these edifices start to crumble? Cixous does not say. It is as if she is telling us that we can make no assumptions, that the only thing we can do is wait and see.

"Woman must write herself and bring woman into literature."

Hélène Cixous

